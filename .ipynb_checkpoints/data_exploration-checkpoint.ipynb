{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# data: q, cq, (dq), (pq), y, *x, *cx\n",
    "# shared: x, cx, (dx), (px), word_counter, char_counter, word2vec\n",
    "# no metadata\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from squad.utils import get_word_span, get_word_idx, process_tokens\n",
    "\n",
    "\n",
    "\n",
    "home = os.path.expanduser(\"~/Desktop/ds1012_final_project\")\n",
    "source_dir = os.path.join(home, \"data\", \"squad\")\n",
    "target_dir = \"data/squad\"\n",
    "glove_dir = os.path.join(home, \"data\", \"glove\")\n",
    "\n",
    "\n",
    "train_ratio =0.9 \n",
    "glove_corpus = \"6B\"\n",
    "glove_vec_size =100\n",
    "mode = \"full\"\n",
    "single_path =\"\" \n",
    "tokenizer =\"PTB\"\n",
    "url =\"vision-server2.corp.ai2\"\n",
    "port =8000\n",
    "split = 'store_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_all():\n",
    "    out_path = os.path.join(source_dir, \"all-v1.1.json\")\n",
    "    if os.path.exists(out_path):\n",
    "        return\n",
    "    train_path = os.path.join(source_dir, \"train-v1.1.json\")\n",
    "    train_data = json.load(open(train_path, 'r'))\n",
    "    dev_path = os.path.join(source_dir, \"dev-v1.1.json\")\n",
    "    dev_data = json.load(open(dev_path, 'r'))\n",
    "    train_data['data'].extend(dev_data['data'])\n",
    "    print(\"dumping all data ...\")\n",
    "    json.dump(train_data, open(out_path, 'w'))\n",
    "\n",
    "\n",
    "def prepro():\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    if mode == 'full':\n",
    "        prepro_each('train', out_name='train')\n",
    "        prepro_each('dev', out_name='dev')\n",
    "        prepro_each('dev', out_name='test')\n",
    "    elif mode == 'all':\n",
    "        create_all()\n",
    "        prepro_each( 'dev', 0.0, 0.0, out_name='dev')\n",
    "        prepro_each( 'dev', 0.0, 0.0, out_name='test')\n",
    "        prepro_each('all', out_name='train')\n",
    "    elif mode == 'single':\n",
    "        assert len(single_path) > 0\n",
    "        prepro_each(\"NULL\", out_name=\"single\", in_path=single_path)\n",
    "    else:\n",
    "        prepro_each('train', 0.0, train_ratio, out_name='train')\n",
    "        prepro_each('train', train_ratio, 1.0, out_name='dev')\n",
    "        prepro_each('dev', out_name='test')\n",
    "\n",
    "\n",
    "def save(data, shared, data_type):\n",
    "    data_path = os.path.join(target_dir, \"data_{}.json\".format(data_type))\n",
    "    shared_path = os.path.join(target_dir, \"shared_{}.json\".format(data_type))\n",
    "    json.dump(data, open(data_path, 'w'))\n",
    "    json.dump(shared, open(shared_path, 'w'))\n",
    "\n",
    "\n",
    "def get_word2vec(word_counter):\n",
    "    glove_path = os.path.join(glove_dir, \"glove.{}.{}d.txt\".format(glove_corpus, glove_vec_size))\n",
    "    sizes = {'6B': int(4e5), '42B': int(1.9e6), '840B': int(2.2e6), '2B': int(1.2e6)}\n",
    "    total = sizes[glove_corpus]\n",
    "    word2vec_dict = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as fh:\n",
    "        for line in tqdm(fh, total=total):\n",
    "            array = line.lstrip().rstrip().split(\" \")\n",
    "            word = array[0]\n",
    "            vector = list(map(float, array[1:]))\n",
    "            if word in word_counter:\n",
    "                word2vec_dict[word] = vector\n",
    "            elif word.capitalize() in word_counter:\n",
    "                word2vec_dict[word.capitalize()] = vector\n",
    "            elif word.lower() in word_counter:\n",
    "                word2vec_dict[word.lower()] = vector\n",
    "            elif word.upper() in word_counter:\n",
    "                word2vec_dict[word.upper()] = vector\n",
    "\n",
    "    print(\"{}/{} of word vocab have corresponding vectors in {}\".format(len(word2vec_dict), len(word_counter), glove_path))\n",
    "    return word2vec_dict\n",
    "\n",
    "\n",
    "def prepro_each(data_type, start_ratio=0.0, stop_ratio=1.0, out_name=\"default\", in_path=None):\n",
    "    if tokenizer == \"PTB\":\n",
    "        import nltk\n",
    "        sent_tokenize = nltk.sent_tokenize\n",
    "        def word_tokenize(tokens):\n",
    "            return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "    elif tokenizer == 'Stanford':\n",
    "        from my.corenlp_interface import CoreNLPInterface\n",
    "        interface = CoreNLPInterface(url, port)\n",
    "        sent_tokenize = interface.split_doc\n",
    "        word_tokenize = interface.split_sent\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    if not split:\n",
    "        sent_tokenize = lambda para: [para]\n",
    "\n",
    "    source_path = in_path or os.path.join(\n",
    "        source_dir, \"{}-v1.1.json\".format(data_type))\n",
    "    source_data = json.load(open(source_path, 'r'))\n",
    "\n",
    "    q, cq, y, rx, rcx, ids, idxs = [], [], [], [], [], [], []\n",
    "    cy = []\n",
    "    x, cx = [], []\n",
    "    answerss = []\n",
    "    p = []\n",
    "    word_counter, char_counter, lower_word_counter = Counter(), Counter(), Counter()\n",
    "    start_ai = int(round(len(source_data['data']) * start_ratio))\n",
    "    stop_ai = int(round(len(source_data['data']) * stop_ratio))\n",
    "    for ai, article in enumerate(tqdm(source_data['data'][start_ai:stop_ai])):\n",
    "        xp, cxp = [], []\n",
    "        pp = []\n",
    "        x.append(xp)\n",
    "        cx.append(cxp)\n",
    "        p.append(pp)\n",
    "        for pi, para in enumerate(article['paragraphs']):\n",
    "            # wordss\n",
    "            context = para['context']\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "            xi = list(map(word_tokenize, sent_tokenize(context)))\n",
    "            xi = [process_tokens(tokens) for tokens in xi]  # process tokens\n",
    "            # given xi, add chars\n",
    "            cxi = [[list(xijk) for xijk in xij] for xij in xi]\n",
    "            xp.append(xi)\n",
    "            cxp.append(cxi)\n",
    "            pp.append(context)\n",
    "\n",
    "            for xij in xi:\n",
    "                for xijk in xij:\n",
    "                    word_counter[xijk] += len(para['qas'])\n",
    "                    lower_word_counter[xijk.lower()] += len(para['qas'])\n",
    "                    for xijkl in xijk:\n",
    "                        char_counter[xijkl] += len(para['qas'])\n",
    "\n",
    "            rxi = [ai, pi]\n",
    "            assert len(x) - 1 == ai\n",
    "            assert len(x[ai]) - 1 == pi\n",
    "            for qa in para['qas']:\n",
    "                # get words\n",
    "                qi = word_tokenize(qa['question'])\n",
    "                cqi = [list(qij) for qij in qi]\n",
    "                yi = []\n",
    "                cyi = []\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    answer_text = answer['text']\n",
    "                    answers.append(answer_text)\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_stop = answer_start + len(answer_text)\n",
    "                    # TODO : put some function that gives word_start, word_stop here\n",
    "                    yi0, yi1 = get_word_span(context, xi, answer_start, answer_stop)\n",
    "                    # yi0 = answer['answer_word_start'] or [0, 0]\n",
    "                    # yi1 = answer['answer_word_stop'] or [0, 1]\n",
    "                    assert len(xi[yi0[0]]) > yi0[1]\n",
    "                    assert len(xi[yi1[0]]) >= yi1[1]\n",
    "                    w0 = xi[yi0[0]][yi0[1]]\n",
    "                    w1 = xi[yi1[0]][yi1[1]-1]\n",
    "                    i0 = get_word_idx(context, xi, yi0)\n",
    "                    i1 = get_word_idx(context, xi, (yi1[0], yi1[1]-1))\n",
    "                    cyi0 = answer_start - i0\n",
    "                    cyi1 = answer_stop - i1 - 1\n",
    "                    # print(answer_text, w0[cyi0:], w1[:cyi1+1])\n",
    "                    assert answer_text[0] == w0[cyi0], (answer_text, w0, cyi0)\n",
    "                    assert answer_text[-1] == w1[cyi1]\n",
    "                    assert cyi0 < 32, (answer_text, w0)\n",
    "                    assert cyi1 < 32, (answer_text, w1)\n",
    "\n",
    "                    yi.append([yi0, yi1])\n",
    "                    cyi.append([cyi0, cyi1])\n",
    "\n",
    "                for qij in qi:\n",
    "                    word_counter[qij] += 1\n",
    "                    lower_word_counter[qij.lower()] += 1\n",
    "                    for qijk in qij:\n",
    "                        char_counter[qijk] += 1\n",
    "\n",
    "                q.append(qi)\n",
    "                cq.append(cqi)\n",
    "                y.append(yi)\n",
    "                cy.append(cyi)\n",
    "                rx.append(rxi)\n",
    "                rcx.append(rxi)\n",
    "                ids.append(qa['id'])\n",
    "                idxs.append(len(idxs))\n",
    "                answerss.append(answers)\n",
    "\n",
    "\n",
    "    word2vec_dict = get_word2vec(word_counter)\n",
    "    lower_word2vec_dict = get_word2vec(lower_word_counter)\n",
    "\n",
    "    # add context here\n",
    "    data = {'q': q, 'cq': cq, 'y': y, '*x': rx, '*cx': rcx, 'cy': cy,\n",
    "            'idxs': idxs, 'ids': ids, 'answerss': answerss, '*p': rx}\n",
    "    shared = {'x': x, 'cx': cx, 'p': p,\n",
    "              'word_counter': word_counter, 'char_counter': char_counter, 'lower_word_counter': lower_word_counter,\n",
    "              'word2vec': word2vec_dict, 'lower_word2vec': lower_word2vec_dict}\n",
    "\n",
    "    print(\"saving ...\")\n",
    "    save(data, shared, out_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [01:42<00:00,  4.33it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glove_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2a703618f064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-352f82a37745>\u001b[0m in \u001b[0;36mprepro\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprepro_each\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprepro_each\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprepro_each\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-352f82a37745>\u001b[0m in \u001b[0;36mprepro_each\u001b[0;34m(data_type, start_ratio, stop_ratio, out_name, in_path)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mword2vec_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0mlower_word2vec_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_word_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-352f82a37745>\u001b[0m in \u001b[0;36mget_word2vec\u001b[0;34m(word_counter)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"glove.{}.{}d.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_vec_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'6B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'42B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.9e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'840B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.2e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.2e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglove_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glove_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "prepro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
