{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for DRQA data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import msgpack\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import tqdm\n",
    "def clean_spaces(text):\n",
    "    \"\"\"normalize spaces in a string.\"\"\"\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iob_np_tag(tag_list):\n",
    "    '''\n",
    "    @in: a list of POS tags\n",
    "    @out: iob_np tags\n",
    "    '''\n",
    "    iob_np = ['o_np'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if 'NN' in tag_list[i]:\n",
    "            if iob_np[i-1] == 'b_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            elif iob_np[i-1] == 'i_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            else:\n",
    "                iob_np[i] = 'b_np'       \n",
    "        i +=1\n",
    "    return iob_np\n",
    "\n",
    "def iob_ner_tag(tag_list):\n",
    "    '''\n",
    "    @in: a list of ner tags\n",
    "    @out: iob_ner tags\n",
    "    '''\n",
    "    iob_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0:\n",
    "            if iob_ner[i-1] == 'b_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            elif iob_ner[i-1] == 'i_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            else:\n",
    "                iob_ner[i] = 'b_ner'       \n",
    "        i +=1\n",
    "    return iob_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global nlp\n",
    "nlp = spacy.load('en', parser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'CD',\n",
       " '.',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_doc = nlp('Microsoft Corporation is a technology company founded in 1975. This corporation develops computer software.')\n",
    "[w.tag_ for w in c_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ORG', 'ORG', '', '', '', '', '', '', 'DATE', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.ent_type_ for w in c_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the building has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'Architecturally, the building has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question = 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "q_doc = nlp(clean_spaces(question))\n",
    "c_doc = nlp(clean_spaces(context))\n",
    "question_tokens = [normalize_text(w.text) for w in q_doc]\n",
    "context_tokens = [normalize_text(w.text) for w in c_doc]\n",
    "question_tokens_lower = [w.lower() for w in question_tokens]\n",
    "context_tokens_lower = [w.lower() for w in context_tokens]\n",
    "context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc] # the lenghth of each tokens\n",
    "context_tags = [w.tag_ for w in c_doc] # POS tagging\n",
    "context_ents = [w.ent_type_ for w in c_doc] # NER tagging\n",
    "context_iob_np = iob_np_tag(context_tags)\n",
    "context_iob_ner = iob_ner_tag(context_ents)\n",
    "\n",
    "question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in q_doc}\n",
    "# PRON is such as me/it/you\n",
    "# lemma_ : cats -> cat\n",
    "\n",
    "question_tokens_set = set(question_tokens)\n",
    "question_tokens_lower_set = set(question_tokens_lower)\n",
    "match_origin = [w in question_tokens_set for w in context_tokens]\n",
    "match_lower = [w in question_tokens_lower_set for w in context_tokens_lower]\n",
    "match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in c_doc]\n",
    "# term frequency in document\n",
    "counter_ = collections.Counter(context_tokens_lower)\n",
    "total = len(context_tokens_lower)\n",
    "context_tf = [counter_[w] / total for w in context_tokens_lower]\n",
    "context_features = list(zip(match_origin, match_lower, match_lemma, context_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Catholic Main Building Main Building Christ Venite Ad Me Omnes Main Building Basilica Sacred Heart Grotto Marian Lourdes France Mary Saint Bernadette Soubirous 1858 end main drive 3 Dome Mary"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner_context_lemma + ner_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'an', 'the', 'of', 'for', '\\'s', ]\n",
    "def part_ner_tag(tag_list, context_list):\n",
    "    '''\n",
    "    @in: a list of ner tags\n",
    "    @out: part of ner tags\n",
    "    '''\n",
    "    ner_context = []\n",
    "    part_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0 and context_list[i] not in stop_words:\n",
    "            part_ner[i] = 'i_ner'\n",
    "            ner_context.append(context_list[i])\n",
    "    \n",
    "    # combine lemma to ner_context list\n",
    "    ner_context_str = ' '.join(ner_context)\n",
    "    ner_context_ = nlp(ner_context_str)\n",
    "    ner_context_lemma = [w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in ner_context_]\n",
    "    ner_context_all = ner_context_lemma +  ner_context\n",
    "        \n",
    "    for j in range(len(context_list)):\n",
    "        if context_list[j] in ner_context_all:\n",
    "            part_ner[j] = 'i_ner'\n",
    "    return part_ner, ner_context_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_ner ,ner_context_all= part_ner_tag(context_ents, context_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0                1      2\n",
      "0                 Architecturally  o_ner\n",
      "1                               ,  o_ner\n",
      "2                             the  o_ner\n",
      "3                        building  i_ner\n",
      "4                             has  o_ner\n",
      "5                               a  o_ner\n",
      "6           NORP         Catholic  i_ner\n",
      "7                       character  o_ner\n",
      "8                               .  o_ner\n",
      "9                            Atop  o_ner\n",
      "10           FAC              the  o_ner\n",
      "11           FAC             Main  i_ner\n",
      "12           FAC         Building  i_ner\n",
      "13           FAC               's  o_ner\n",
      "14                           gold  o_ner\n",
      "15                           dome  i_ner\n",
      "16                             is  o_ner\n",
      "17                              a  o_ner\n",
      "18                         golden  o_ner\n",
      "19                         statue  o_ner\n",
      "20                             of  o_ner\n",
      "21                            the  o_ner\n",
      "22                         Virgin  o_ner\n",
      "23                           Mary  i_ner\n",
      "24                              .  o_ner\n",
      "25                    Immediately  o_ner\n",
      "26                             in  o_ner\n",
      "27                          front  o_ner\n",
      "28                             of  o_ner\n",
      "29           FAC              the  o_ner\n",
      "30           FAC             Main  i_ner\n",
      "31           FAC         Building  i_ner\n",
      "32                            and  o_ner\n",
      "33                         facing  o_ner\n",
      "34                             it  o_ner\n",
      "35                              ,  o_ner\n",
      "36                             is  o_ner\n",
      "37                              a  o_ner\n",
      "38                         copper  o_ner\n",
      "39                         statue  o_ner\n",
      "40                             of  o_ner\n",
      "41          NORP           Christ  i_ner\n",
      "42                           with  o_ner\n",
      "43                           arms  o_ner\n",
      "44                       upraised  o_ner\n",
      "45                           with  o_ner\n",
      "46                            the  o_ner\n",
      "47                         legend  o_ner\n",
      "48                              \"  o_ner\n",
      "49   WORK_OF_ART           Venite  i_ner\n",
      "50   WORK_OF_ART               Ad  i_ner\n",
      "51   WORK_OF_ART               Me  i_ner\n",
      "52   WORK_OF_ART            Omnes  i_ner\n",
      "53                              \"  o_ner\n",
      "54                              .  o_ner\n",
      "55                           Next  o_ner\n",
      "56                             to  o_ner\n",
      "57           FAC              the  o_ner\n",
      "58           FAC             Main  i_ner\n",
      "59           FAC         Building  i_ner\n",
      "60                             is  o_ner\n",
      "61                            the  o_ner\n",
      "62        PERSON         Basilica  i_ner\n",
      "63                             of  o_ner\n",
      "64           ORG              the  o_ner\n",
      "65           ORG           Sacred  i_ner\n",
      "66           ORG            Heart  i_ner\n",
      "67                              .  o_ner\n",
      "68                    Immediately  o_ner\n",
      "69                         behind  o_ner\n",
      "70                            the  o_ner\n",
      "71                       basilica  i_ner\n",
      "72                             is  o_ner\n",
      "73                            the  o_ner\n",
      "74           GPE           Grotto  i_ner\n",
      "75                              ,  o_ner\n",
      "76                              a  o_ner\n",
      "77        PERSON           Marian  i_ner\n",
      "78                          place  o_ner\n",
      "79                             of  o_ner\n",
      "80                         prayer  o_ner\n",
      "81                            and  o_ner\n",
      "82                     reflection  o_ner\n",
      "83                              .  o_ner\n",
      "84                             It  o_ner\n",
      "85                             is  o_ner\n",
      "86                              a  o_ner\n",
      "87                        replica  o_ner\n",
      "88                             of  o_ner\n",
      "89                            the  o_ner\n",
      "90                         grotto  i_ner\n",
      "91                             at  o_ner\n",
      "92           GPE          Lourdes  i_ner\n",
      "93                              ,  o_ner\n",
      "94           GPE           France  i_ner\n",
      "95                          where  o_ner\n",
      "96                            the  o_ner\n",
      "97                         Virgin  o_ner\n",
      "98        PERSON             Mary  i_ner\n",
      "99                      reputedly  o_ner\n",
      "100                      appeared  o_ner\n",
      "101                            to  o_ner\n",
      "102       PERSON            Saint  i_ner\n",
      "103       PERSON       Bernadette  i_ner\n",
      "104       PERSON        Soubirous  i_ner\n",
      "105                            in  o_ner\n",
      "106         DATE             1858  i_ner\n",
      "107                             .  o_ner\n",
      "108                            At  o_ner\n",
      "109         DATE              the  o_ner\n",
      "110         DATE              end  i_ner\n",
      "111         DATE               of  o_ner\n",
      "112         DATE              the  o_ner\n",
      "113         DATE             main  i_ner\n",
      "114         DATE            drive  i_ner\n",
      "115                             (  o_ner\n",
      "116                           and  o_ner\n",
      "117                            in  o_ner\n",
      "118                             a  o_ner\n",
      "119                        direct  o_ner\n",
      "120                          line  o_ner\n",
      "121                          that  o_ner\n",
      "122                      connects  o_ner\n",
      "123                       through  o_ner\n",
      "124     CARDINAL                3  i_ner\n",
      "125                       statues  o_ner\n",
      "126                           and  o_ner\n",
      "127                           the  o_ner\n",
      "128                          Gold  o_ner\n",
      "129       PERSON             Dome  i_ner\n",
      "130                             )  o_ner\n",
      "131                             ,  o_ner\n",
      "132                            is  o_ner\n",
      "133                             a  o_ner\n",
      "134                        simple  o_ner\n",
      "135                             ,  o_ner\n",
      "136                        modern  o_ner\n",
      "137                         stone  o_ner\n",
      "138                        statue  o_ner\n",
      "139                            of  o_ner\n",
      "140       PERSON             Mary  i_ner\n",
      "141                             .  o_ner\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.column_stack([context_ents, context_tokens, part_ner]))\n",
    "print (df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_ner</th>\n",
       "      <td>NORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_ner</th>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_ner</th>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_ner</th>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_ner</th>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_ner</th>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_ner</th>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_ner</th>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_ner</th>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_ner</th>\n",
       "      <td>CARDINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_ner</th>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_ner</th>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_ner</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "b_ner      NORP\n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "b_ner       FAC\n",
       "i_ner       FAC\n",
       "i_ner       FAC\n",
       "i_ner       FAC\n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "b_ner       FAC\n",
       "...         ...\n",
       "i_ner      DATE\n",
       "i_ner      DATE\n",
       "i_ner      DATE\n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "b_ner  CARDINAL\n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "b_ner    PERSON\n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "o_ner          \n",
       "b_ner    PERSON\n",
       "o_ner          \n",
       "\n",
       "[142 rows x 1 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(context_ents, context_iob_ner)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_answer(row):\n",
    "    token_span = row[-4]\n",
    "    starts, ends = zip(*token_span)\n",
    "    answer_start = row[-2]\n",
    "    answer_end = row[-1]\n",
    "    try:\n",
    "        return row[:-3] + (starts.index(answer_start), ends.index(answer_end))\n",
    "    except ValueError:\n",
    "        return row[:-3] + (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_span = context_token_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starts, ends = zip(*token_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer = 'Saint Bernadette Soubirous'\n",
    "answer_start = 515\n",
    "answer_end = answer_start + len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts.index(answer_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends.index(answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos tagging count for context\n",
    "counter_tag = collections.Counter(w for w in context_tags) #context_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"''\": 1,\n",
       "         ',': 6,\n",
       "         '-LRB-': 1,\n",
       "         '-RRB-': 1,\n",
       "         '.': 7,\n",
       "         'CC': 4,\n",
       "         'CD': 2,\n",
       "         'DT': 22,\n",
       "         'IN': 20,\n",
       "         'JJ': 7,\n",
       "         'NN': 20,\n",
       "         'NNP': 27,\n",
       "         'NNS': 2,\n",
       "         'POS': 1,\n",
       "         'PRP': 3,\n",
       "         'RB': 5,\n",
       "         'VBD': 1,\n",
       "         'VBG': 1,\n",
       "         'VBZ': 8,\n",
       "         'WDT': 1,\n",
       "         'WRB': 1,\n",
       "         '``': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_tag = sorted(counter_tag, key=counter_tag.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag2id = {w: i for i, w in enumerate(vocab_tag)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\": 16,\n",
       " ',': 7,\n",
       " '-LRB-': 19,\n",
       " '-RRB-': 21,\n",
       " '.': 6,\n",
       " 'CC': 9,\n",
       " 'CD': 12,\n",
       " 'DT': 1,\n",
       " 'IN': 3,\n",
       " 'JJ': 5,\n",
       " 'NN': 2,\n",
       " 'NNP': 0,\n",
       " 'NNS': 11,\n",
       " 'POS': 13,\n",
       " 'PRP': 10,\n",
       " 'RB': 8,\n",
       " 'VBD': 18,\n",
       " 'VBG': 14,\n",
       " 'VBZ': 4,\n",
       " 'WDT': 20,\n",
       " 'WRB': 17,\n",
       " '``': 15}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# largest count with small index number\n",
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import msgpack\n",
    "with open('SQuAD/meta.msgpack', 'rb') as f:\n",
    "    meta = msgpack.load(f, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "embedding = torch.Tensor(meta['char_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87603, 100])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o_ner', 'i_ner']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta['vocab_part_ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of pos tag given by spacy \n",
    "len(meta['vocab_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of NER tag given by spacy\n",
    "len(meta['vocab_ent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pos tag\n",
    "#meta['vocab_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#meta['vocab_ent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context\n",
    "data['train'][0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example cotext id \n",
    "#data['train'][0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for example pos tag_id\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data['train'][0][3], data['train'][0][5])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta['vocab'][53946]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RB'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta['vocab_tag'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta['embedding'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(meta['vocab']) == len(meta['char_embeddings']) ==91187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 874474\n",
    "d_emb = 100\n",
    "seen = set()\n",
    "fin_name = 'char/charNgram.txt'\n",
    "with open(fin_name, 'r') as ftxt:\n",
    "    content = ftxt.read()\n",
    "    lines = content.splitlines()\n",
    "    batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    elems = line.rstrip().split()\n",
    "    vec = [float(n) for n in elems[-d_emb:]]\n",
    "    word = ' '.join(elems[:-d_emb])\n",
    "    if word in seen:\n",
    "        continue\n",
    "    seen.add(word)\n",
    "    batch.append((word, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(sentence, n):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        list: a list of lists of words corresponding to the ngrams in the sentence.\n",
    "    \"\"\"\n",
    "    return [sentence[i:i+n] for i in range(len(sentence)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb(w, default='zero'):\n",
    "    assert default == 'zero', 'only zero default is supported for character embeddings'\n",
    "    chars = ['#BEGIN#'] + list(w) + ['#END#']\n",
    "    embs = np.zeros(d_emb, dtype=np.float32)\n",
    "    match = {}\n",
    "    for i in [2, 3, 4]:\n",
    "        grams = ngrams(chars, i)\n",
    "        for g in grams:\n",
    "            g = '{}gram-{}'.format(i, ''.join(g))\n",
    "            e = self.lookup(g)\n",
    "            if e is not None:\n",
    "                match[g] = np.array(e, np.float32)\n",
    "    if match:\n",
    "        embs = sum(match.values()) / len(match)\n",
    "    return embs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = ['#BEGIN#'] + list('cat') + ['#END#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embs = np.zeros(d_emb, dtype=np.float32)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams = ngrams(chars, i)\n",
    "    for g in grams:\n",
    "        g = '{}gram-{}'.format(i, ''.join(g))\n",
    "        print(g)\n",
    "        #e = lookup(g)\n",
    "        #if e is not None:\n",
    "        #    match[g] = np.array(e, np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_file = 'SQuAD/train-v1.1.json'\n",
    "import json\n",
    "\n",
    "def flatten_json(data_file, mode):\n",
    "    \"\"\"Flatten each article in training data.\"\"\"\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)['data']\n",
    "    rows = []\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                id_, question, answers = qa['id'], qa['question'], qa['answers']\n",
    "                if mode == 'train':\n",
    "                    answer = answers[0]['text']  # in training data there's only one answer\n",
    "                    answer_start = answers[0]['answer_start'] # char level length\n",
    "                    answer_end = answer_start + len(answer) # char level lenght\n",
    "                    rows.append((id_, context, question, answer, answer_start, answer_end))\n",
    "                else:  # mode == 'dev'\n",
    "                    answers = [a['text'] for a in answers]\n",
    "                    rows.append((id_, context, question, answers))\n",
    "    return rows\n",
    "\n",
    "\n",
    "train = flatten_json(trn_file, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5733bf84d058e614000b61bd',\n",
       " \"As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\",\n",
       " 'How many student news papers are found at Notre Dame?',\n",
       " 'three',\n",
       " 126,\n",
       " 131)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_vocab = set()\n",
    "with open('glove/glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        token = normalize_text(line.rstrip().split(' ')[0])\n",
    "        wv_vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2195960"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(questions, contexts):\n",
    "    \"\"\"\n",
    "    Build vocabulary sorted by global word frequency, or consider frequencies in questions first,\n",
    "    which is controlled by `args.sort_all`.\n",
    "    \"\"\"\n",
    "    if True:\n",
    "        counter = collections.Counter(w for doc in questions + contexts for w in doc)\n",
    "        vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True)\n",
    "    else:\n",
    "        counter_q = collections.Counter(w for doc in questions for w in doc)\n",
    "        counter_c = collections.Counter(w for doc in contexts for w in doc)\n",
    "        counter = counter_c + counter_q\n",
    "        vocab = sorted([t for t in counter_q if t in wv_vocab], key=counter_q.get, reverse=True)\n",
    "        vocab += sorted([t for t in counter_c.keys() - counter_q.keys() if t in wv_vocab],\n",
    "                        key=counter.get, reverse=True)\n",
    "    total = sum(counter.values())\n",
    "    matched = sum(counter[t] for t in vocab)\n",
    "    vocab.insert(0, \"<PAD>\") # in question_id and context_id, the 0 means padding\n",
    "    vocab.insert(1, \"<UNK>\")\n",
    "    return vocab, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = collections.Counter(w for doc in [row[5]] + [row[1]] for w in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = sum(counter.values())\n",
    "matched = sum(counter[t] for t in vocab)\n",
    "matched == total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = train\n",
    "full = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab, counter = build_vocab([row[5]], [row[1]])\n",
    "counter_tag = collections.Counter(w for w in row[3]) #context_tags\n",
    "vocab_tag = sorted(counter_tag, key=counter_tag.get, reverse=True) # high rank with larger count\n",
    "counter_ent = collections.Counter(w for w in row[4])\n",
    "vocab_ent = sorted(counter_ent, key=counter_ent.get, reverse=True)\n",
    "w2id = {w: i for i, w in enumerate(vocab)}\n",
    "tag2id = {w: i for i, w in enumerate(vocab_tag)} # larger count(hight rank) with small index\n",
    "ent2id = {w: i for i, w in enumerate(vocab_ent)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_id(row, unk_id=1):\n",
    "    context_tokens = row[1]\n",
    "    context_features = row[2]\n",
    "    context_tags = row[3]\n",
    "    context_ents = row[4]\n",
    "    question_tokens = row[5]\n",
    "    question_ids = [w2id[w] if w in w2id else unk_id for w in question_tokens]\n",
    "    context_ids = [w2id[w] if w in w2id else unk_id for w in context_tokens]\n",
    "    tag_ids = [tag2id[w] for w in context_tags]\n",
    "    ent_ids = [ent2id[w] for w in context_ents]\n",
    "    return (row[0], context_ids, context_features, tag_ids, ent_ids, question_ids) + row[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ann_id = to_id(train, unk_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embeddings = np.zeros((vocab_size, 300))\n",
    "embed_counts = np.zeros(vocab_size)\n",
    "embed_counts[:2] = 1  # PADDING & UNK\n",
    "wv_file = 'glove/glove.840B.300d.txt'\n",
    "with open(wv_file) as f:\n",
    "    for line in f:\n",
    "        elems = line.rstrip().split(' ')\n",
    "        token = normalize_text(elems[0])\n",
    "        if token in w2id:\n",
    "            word_id = w2id[token]\n",
    "            embed_counts[word_id] += 1\n",
    "            embeddings[word_id] += [float(v) for v in elems[1:]]\n",
    "embeddings /= embed_counts.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))\n",
    "print(len(embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add char embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(sentence, n):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        list: a list of lists of words corresponding to the ngrams in the sentence.\n",
    "    \"\"\"\n",
    "    return [sentence[i:i+n] for i in range(len(sentence)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from embeddings.embedding import Embedding\n",
    "class CharEmbedding(Embedding):\n",
    "\n",
    "    size = 874474\n",
    "    d_emb = 100\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.db = self.initialize_db(self.path('char/kazuma.db'))\n",
    "        if len(self) < self.size:\n",
    "            self.clear()\n",
    "            self.load_word2emb()\n",
    "            \n",
    "\n",
    "    def emb(self, w, default='zero'):\n",
    "        assert default == 'zero', 'only zero default is supported for character embeddings'\n",
    "        chars = ['#BEGIN#'] + list(w) + ['#END#']\n",
    "        embs = np.zeros(self.d_emb, dtype=np.float32)\n",
    "        match = {}\n",
    "        for i in [2, 3, 4]:\n",
    "            grams = ngrams(chars, i)\n",
    "            for g in grams:\n",
    "                g = '{}gram-{}'.format(i, ''.join(g))\n",
    "                e = self.lookup(g)\n",
    "                if e is not None:\n",
    "                    match[g] = np.array(e, np.float32)\n",
    "        if match:\n",
    "            embs = sum(match.values()) / len(match)\n",
    "        return embs.tolist()\n",
    "\n",
    "    def load_word2emb(self, batch_size=1000):\n",
    "        seen = set()\n",
    "        fin_name = 'char/charNgram.txt'\n",
    "        with open(fin_name, 'r') as ftxt:\n",
    "            content = ftxt.read()\n",
    "            lines = content.splitlines()\n",
    "            batch = []\n",
    "            for line in lines:\n",
    "                elems = line.rstrip().split()\n",
    "                vec = [float(n) for n in elems[-d_emb:]]\n",
    "                word = ' '.join(elems[:-d_emb])\n",
    "                if word in seen:\n",
    "                    continue\n",
    "                seen.add(word)                \n",
    "                batch.append((word, vec))\n",
    "                if len(batch) == batch_size:\n",
    "                    self.insert_batch(batch)\n",
    "                    batch.clear()\n",
    "            if batch:\n",
    "                self.insert_batch(batch)\n",
    "charembedding = CharEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "char_embeddings = np.zeros((vocab_size, 100))\n",
    "char_embed_counts = np.zeros(vocab_size)\n",
    "char_embed_counts[:2] = 1  # PADDING & UNK\n",
    "for token in w2id:\n",
    "    word_id = w2id[token]\n",
    "    char_embed_counts[word_id] += 1\n",
    "    char_embeddings[word_id] += charembedding.emb(token) \n",
    "char_embeddings /= char_embed_counts.reshape((-1, 1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_char_embedding = np.concatenate((embeddings, char_embeddings), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_char_embedding[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = glove_char_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 5, 6],\n",
       "       [3, 4, 1, 1, 2]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 1], [3, 4, 1]])\n",
    "b = np.array([[5, 6], [1,2]])\n",
    "np.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1],\n",
       "       [3, 4, 1]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test new added feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(text):\n",
    "    \"\"\"normalize spaces in a string.\"\"\"\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "nlp = None\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize spacy in each process\"\"\"\n",
    "    '''\n",
    "    'en': Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head.\n",
    "    parser=False or disable=['parser'] : don't need any of the syntactic information,\n",
    "                                        and will make spaCy load and run much faster.\n",
    "    '''\n",
    "    global nlp\n",
    "    nlp = spacy.load('en', parser=False)\n",
    "\n",
    "def annotate(row):\n",
    "    global nlp\n",
    "    id_, context, question = row[:3]\n",
    "    q_doc = nlp(clean_spaces(question))\n",
    "    c_doc = nlp(clean_spaces(context))\n",
    "    question_tokens = [normalize_text(w.text) for w in q_doc]\n",
    "    context_tokens = [normalize_text(w.text) for w in c_doc]\n",
    "    question_tokens_lower = [w.lower() for w in question_tokens]\n",
    "    context_tokens_lower = [w.lower() for w in context_tokens]\n",
    "    context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc] # the lenghth of each tokens\n",
    "    context_tags = [w.tag_ for w in c_doc] # POS tagging\n",
    "    context_ents = [w.ent_type_ for w in c_doc] # NER tagging\n",
    "\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in q_doc}\n",
    "    # PRON is such as me/it/you\n",
    "    # lemma_ : cats -> cat\n",
    "\n",
    "    question_tokens_set = set(question_tokens)\n",
    "    question_tokens_lower_set = set(question_tokens_lower)\n",
    "    match_origin = [w in question_tokens_set for w in context_tokens]\n",
    "    match_lower = [w in question_tokens_lower_set for w in context_tokens_lower]\n",
    "    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in c_doc]\n",
    "    # term frequency in document\n",
    "    counter_ = collections.Counter(context_tokens_lower)\n",
    "    total = len(context_tokens_lower)\n",
    "    # frequent feature\n",
    "    context_tf = [counter_[w] / total for w in context_tokens_lower]\n",
    "    # exact match feature refering to the paper\n",
    "    context_features = list(zip(match_origin, match_lower, match_lemma, context_tf))\n",
    "    if not True:\n",
    "        context_tokens = context_tokens_lower\n",
    "        question_tokens = question_tokens_lower\n",
    "    return (id_, context_tokens, context_features, context_tags, context_ents,\n",
    "            question_tokens, context, context_token_span) + row[3:]\n",
    "def index_answer(row):\n",
    "    token_span = row[-4] #context_token_span\n",
    "    starts, ends = zip(*token_span)\n",
    "    answer_start = row[-2]\n",
    "    answer_end = row[-1]\n",
    "    try:\n",
    "        return row[:-3] + (starts.index(answer_start), ends.index(answer_end))\n",
    "    except ValueError:\n",
    "        return row[:-3] + (None, None)\n",
    "nlp = spacy.load('en', parser=False)\n",
    "train_ann = annotate(train[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How',\n",
       " 'many',\n",
       " 'student',\n",
       " 'news',\n",
       " 'papers',\n",
       " 'are',\n",
       " 'found',\n",
       " 'at',\n",
       " 'Notre',\n",
       " 'Dame',\n",
       " '?']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ann[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SQuAD/data.msgpack', 'rb') as f:\n",
    "    data = msgpack.load(f, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1              2\n",
      "0    0  0             As\n",
      "1    0  0             at\n",
      "2    0  0           most\n",
      "3    0  0          other\n",
      "4    0  0   universities\n",
      "5    0  0              ,\n",
      "6    1  1          Notre\n",
      "7    2  1           Dame\n",
      "8    2  0             's\n",
      "9    0  0       students\n",
      "10   0  0            run\n",
      "11   0  0              a\n",
      "12   0  0         number\n",
      "13   0  0             of\n",
      "14   0  0           news\n",
      "15   0  0          media\n",
      "16   0  0        outlets\n",
      "17   0  0              .\n",
      "18   0  0            The\n",
      "19   1  1           nine\n",
      "20   0  0        student\n",
      "21   0  0              -\n",
      "22   0  0            run\n",
      "23   0  0        outlets\n",
      "24   0  0        include\n",
      "25   1  1          three\n",
      "26   0  0     newspapers\n",
      "27   0  0              ,\n",
      "28   0  0           both\n",
      "29   0  0              a\n",
      "30   0  0          radio\n",
      "31   0  0            and\n",
      "32   0  0     television\n",
      "33   0  0        station\n",
      "34   0  0              ,\n",
      "35   0  0            and\n",
      "36   0  0        several\n",
      "37   0  0      magazines\n",
      "38   0  0            and\n",
      "39   0  0       journals\n",
      "40   0  0              .\n",
      "41   1  1          Begun\n",
      "42   0  0             as\n",
      "43   0  0              a\n",
      "44   1  1            one\n",
      "45   0  0              -\n",
      "46   0  0           page\n",
      "47   0  0        journal\n",
      "48   0  0             in\n",
      "49   1  1      September\n",
      "50   2  1           1876\n",
      "51   0  0              ,\n",
      "52   0  0            the\n",
      "53   1  1     Scholastic\n",
      "54   0  0       magazine\n",
      "55   0  0             is\n",
      "56   0  0         issued\n",
      "57   0  1          twice\n",
      "58   1  1        monthly\n",
      "59   0  0            and\n",
      "60   0  0         claims\n",
      "61   0  0             to\n",
      "62   0  0             be\n",
      "63   0  0            the\n",
      "64   0  0         oldest\n",
      "65   0  0     continuous\n",
      "66   0  0     collegiate\n",
      "67   0  0    publication\n",
      "68   0  0             in\n",
      "69   1  0            the\n",
      "70   2  1         United\n",
      "71   2  1         States\n",
      "72   0  0              .\n",
      "73   0  0            The\n",
      "74   0  0          other\n",
      "75   0  0       magazine\n",
      "76   0  0              ,\n",
      "77   1  0            The\n",
      "78   2  1        Juggler\n",
      "79   0  0              ,\n",
      "80   0  0             is\n",
      "81   0  0       released\n",
      "82   1  1          twice\n",
      "83   2  0              a\n",
      "84   2  1           year\n",
      "85   0  0            and\n",
      "86   0  0        focuses\n",
      "87   0  0             on\n",
      "88   0  0        student\n",
      "89   0  0     literature\n",
      "90   0  0            and\n",
      "91   0  0        artwork\n",
      "92   0  0              .\n",
      "93   0  0            The\n",
      "94   1  1           Dome\n",
      "95   0  0       yearbook\n",
      "96   0  0             is\n",
      "97   0  0      published\n",
      "98   1  1       annually\n",
      "99   0  0              .\n",
      "100  0  0            The\n",
      "101  0  0     newspapers\n",
      "102  0  0           have\n",
      "103  0  0        varying\n",
      "104  0  0    publication\n",
      "105  0  0      interests\n",
      "106  0  0              ,\n",
      "107  0  0           with\n",
      "108  0  0            The\n",
      "109  1  1       Observer\n",
      "110  0  0      published\n",
      "111  1  1          daily\n",
      "112  0  0            and\n",
      "113  0  0         mainly\n",
      "114  0  0      reporting\n",
      "115  0  1     university\n",
      "116  0  0            and\n",
      "117  0  0          other\n",
      "118  0  0           news\n",
      "119  0  0              ,\n",
      "120  0  0            and\n",
      "121  0  0        staffed\n",
      "122  0  0             by\n",
      "123  0  0       students\n",
      "124  0  0           from\n",
      "125  0  0           both\n",
      "126  1  1          Notre\n",
      "127  2  1           Dame\n",
      "128  0  0            and\n",
      "129  1  1          Saint\n",
      "130  2  1           Mary\n",
      "131  2  0             's\n",
      "132  0  0        College\n",
      "133  0  0              .\n",
      "134  0  0         Unlike\n",
      "135  1  1     Scholastic\n",
      "136  0  0            and\n",
      "137  0  0            The\n",
      "138  1  1           Dome\n",
      "139  0  0              ,\n",
      "140  0  0            The\n",
      "141  1  1       Observer\n",
      "142  0  0             is\n",
      "143  0  0             an\n",
      "144  0  0    independent\n",
      "145  0  0    publication\n",
      "146  0  0            and\n",
      "147  0  0           does\n",
      "148  0  0            not\n",
      "149  0  0           have\n",
      "150  0  0              a\n",
      "151  0  0        faculty\n",
      "152  0  0        advisor\n",
      "153  0  0             or\n",
      "154  0  0            any\n",
      "155  0  0      editorial\n",
      "156  0  0      oversight\n",
      "157  0  0           from\n",
      "158  1  0            the\n",
      "159  2  1     University\n",
      "160  0  0              .\n",
      "161  0  0             In\n",
      "162  1  1           1987\n",
      "163  0  0              ,\n",
      "164  0  0           when\n",
      "165  0  0           some\n",
      "166  0  0       students\n",
      "167  0  0       believed\n",
      "168  0  0           that\n",
      "169  1  0            The\n",
      "170  2  1       Observer\n",
      "171  0  0          began\n",
      "172  0  0             to\n",
      "173  0  0           show\n",
      "174  0  0              a\n",
      "175  0  0   conservative\n",
      "176  0  0           bias\n",
      "177  0  0              ,\n",
      "178  0  0              a\n",
      "179  0  0        liberal\n",
      "180  0  0      newspaper\n",
      "181  0  0              ,\n",
      "182  1  1         Common\n",
      "183  2  1          Sense\n",
      "184  0  0            was\n",
      "185  0  0      published\n",
      "186  0  0              .\n",
      "187  0  0       Likewise\n",
      "188  0  0              ,\n",
      "189  0  0             in\n",
      "190  1  1           2003\n",
      "191  0  0              ,\n",
      "192  0  0           when\n",
      "193  0  0          other\n",
      "194  0  0       students\n",
      "195  0  0       believed\n",
      "196  0  0           that\n",
      "197  0  0            the\n",
      "198  0  0          paper\n",
      "199  0  0         showed\n",
      "200  0  0              a\n",
      "201  0  0        liberal\n",
      "202  0  0           bias\n",
      "203  0  0              ,\n",
      "204  0  0            the\n",
      "205  0  0   conservative\n",
      "206  0  0          paper\n",
      "207  1  1          Irish\n",
      "208  0  0          Rover\n",
      "209  0  0           went\n",
      "210  0  0           into\n",
      "211  0  0     production\n",
      "212  0  0              .\n",
      "213  0  0        Neither\n",
      "214  0  0          paper\n",
      "215  0  0             is\n",
      "216  0  0      published\n",
      "217  0  0             as\n",
      "218  0  0          often\n",
      "219  0  0             as\n",
      "220  0  0            The\n",
      "221  1  1       Observer\n",
      "222  0  0              ;\n",
      "223  0  0        however\n",
      "224  0  0              ,\n",
      "225  0  0            all\n",
      "226  1  1          three\n",
      "227  0  0            are\n",
      "228  0  0    distributed\n",
      "229  0  0             to\n",
      "230  0  0            all\n",
      "231  0  0       students\n",
      "232  0  0              .\n",
      "233  0  0        Finally\n",
      "234  0  0              ,\n",
      "235  0  0             in\n",
      "236  1  1         Spring\n",
      "237  2  1           2008\n",
      "238  0  0             an\n",
      "239  0  0  undergraduate\n",
      "240  0  0        journal\n",
      "241  0  0            for\n",
      "242  0  0      political\n",
      "243  0  0        science\n",
      "244  0  0       research\n",
      "245  0  0              ,\n",
      "246  0  0         Beyond\n",
      "247  0  0       Politics\n",
      "248  0  0              ,\n",
      "249  0  0           made\n",
      "250  0  0            its\n",
      "251  0  0          debut\n",
      "252  0  0              .\n"
     ]
    }
   ],
   "source": [
    "# train: id, context_id, context_features, tag_id, ent_id, iob_np, iob_ner, part_ner, q tag_id, q ent_id, q iob_np, q iob_ner,\n",
    "#        question_id, context, context_token_span, answer_start, answer_end\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.column_stack([data['train'][8][6],data['train'][8][7], train_ann[1]]))\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3        4\n",
      "0   12  0  0  0      How\n",
      "1    5  0  0  0     many\n",
      "2    0  0  1  0  student\n",
      "3    0  0  2  0     news\n",
      "4    8  0  2  0   papers\n",
      "5   13  0  0  0      are\n",
      "6   10  0  0  0    found\n",
      "7    1  0  0  0       at\n",
      "8    2  2  1  1    Notre\n",
      "9    2  2  2  2     Dame\n",
      "10   3  0  0  0        ?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.column_stack([data['train'][8][8],data['train'][8][9], data['train'][8][10],data['train'][8][11],train_ann[5]]))\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SQuAD/meta.msgpack', 'rb') as f:\n",
    "    meta = msgpack.load(f, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta['vocab_q_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4,4)\n",
    "ee = torch.randn(4,6)\n",
    "b = torch.randn(4,5)\n",
    "c = torch.randn(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = [a,ee]\n",
    "list_.append(b)\n",
    "list_.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " -1.2702 -1.8569  0.4600  0.1064\n",
       "  0.4058  0.1211 -1.3250  0.2720\n",
       "  0.2532  1.7568 -0.8371  0.2591\n",
       " -0.4002  1.0083 -0.9354 -0.4041\n",
       " [torch.FloatTensor of size 4x4], \n",
       " -0.9446 -0.4591  1.5962  1.0960 -0.4257  0.3042\n",
       " -0.7830 -0.7529  0.6747  0.9685 -1.6344 -0.3820\n",
       " -0.5750 -0.3853 -1.4120  2.1387  0.3732  0.9374\n",
       " -2.4007  1.2158 -0.8062 -0.1280 -0.6776 -0.5533\n",
       " [torch.FloatTensor of size 4x6], \n",
       " -0.1084  1.8311 -1.9461 -0.3505 -0.5678\n",
       " -0.3808 -1.8617  1.3735  0.9692  0.3530\n",
       "  0.2152 -0.6362  0.4172 -1.3567  1.0117\n",
       " -0.8054  0.6637  0.5527  0.5326 -1.5344\n",
       " [torch.FloatTensor of size 4x5], \n",
       " -0.9352 -0.1562\n",
       " -0.6677 -0.1769\n",
       "  0.5419 -1.4153\n",
       " -0.6163 -0.2632\n",
       " [torch.FloatTensor of size 4x2]]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "drnn_input = torch.cat(list_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 17])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drnn_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2_pos is torch.Size([4, 11])\n"
     ]
    }
   ],
   "source": [
    "print('x2_pos is {}'.format(drnn_input.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [1,1,1,1,None, None,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [2*e for e in ex[0:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, None, None, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input+ ex[4:6] + [2*e for e in ex[-5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, None, None, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = torch.FloatTensor()\n",
    "len(empty) ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       " \n",
       " Columns 0 to 9 \n",
       " -1.2702 -1.8569  0.4600  0.1064 -0.9446 -0.4591  1.5962  1.0960 -0.4257  0.3042\n",
       "  0.4058  0.1211 -1.3250  0.2720 -0.7830 -0.7529  0.6747  0.9685 -1.6344 -0.3820\n",
       "  0.2532  1.7568 -0.8371  0.2591 -0.5750 -0.3853 -1.4120  2.1387  0.3732  0.9374\n",
       " -0.4002  1.0083 -0.9354 -0.4041 -2.4007  1.2158 -0.8062 -0.1280 -0.6776 -0.5533\n",
       " \n",
       " Columns 10 to 16 \n",
       " -0.1084  1.8311 -1.9461 -0.3505 -0.5678 -0.9352 -0.1562\n",
       " -0.3808 -1.8617  1.3735  0.9692  0.3530 -0.6677 -0.1769\n",
       "  0.2152 -0.6362  0.4172 -1.3567  1.0117  0.5419 -1.4153\n",
       " -0.8054  0.6637  0.5527  0.5326 -1.5344 -0.6163 -0.2632\n",
       " [torch.FloatTensor of size 4x17],\n",
       " Variable containing:[torch.FloatTensor with no dimension],\n",
       " 2,\n",
       " 3,\n",
       " 4]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "[Variable(drnn_input)] + [Variable(empty)] + [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
