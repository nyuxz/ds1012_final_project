{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import msgpack\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(data_file, mode):\n",
    "    \"\"\"Flatten each article in training data.\"\"\"\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)['data']\n",
    "    rows = []\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                id_, question, answers = qa['id'], qa['question'], qa['answers']\n",
    "                if mode == 'train':\n",
    "                    answer = answers[0]['text']  # in training data there's only one answer\n",
    "                    answer_start = answers[0]['answer_start'] # char level length\n",
    "                    answer_end = answer_start + len(answer) # char level lenght\n",
    "                    rows.append((id_, context, question, answer, answer_start, answer_end))\n",
    "                else:  # mode == 'dev'\n",
    "                    answers = [a['text'] for a in answers]\n",
    "                    rows.append((id_, context, question, answers))\n",
    "    return rows\n",
    "\n",
    "def iob_np_tag(tag_list):\n",
    "    '''\n",
    "    function for creating iob_np\n",
    "    @in: a list of POS tags\n",
    "    @out: iob_np tags\n",
    "    '''\n",
    "    iob_np = ['o_np'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if 'NN' in tag_list[i]:\n",
    "            if iob_np[i-1] == 'b_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            elif iob_np[i-1] == 'i_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            else:\n",
    "                iob_np[i] = 'b_np'\n",
    "        i +=1\n",
    "    return iob_np\n",
    "\n",
    "## iob tag for NER\n",
    "def iob_ner_tag(tag_list):\n",
    "    '''\n",
    "    function for creating iob_ner\n",
    "    @in: a list of ner tags\n",
    "    @out: iob_ner tags\n",
    "    '''\n",
    "    iob_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0:\n",
    "            if iob_ner[i-1] == 'b_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            elif iob_ner[i-1] == 'i_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            else:\n",
    "                iob_ner[i] = 'b_ner'\n",
    "        i +=1\n",
    "    return iob_ner\n",
    "\n",
    "## Part of NER tag\n",
    "stop_words = ['a', 'an', 'the', 'of', 'for', '\\'s', 'For', 'The', 'A', 'An', ',', ':', '.', ' ,', ', ']\n",
    "def part_ner_tag(tag_list, context_list):\n",
    "    '''\n",
    "    @in: a list of ner tags\n",
    "    @out: part of ner tags\n",
    "    '''\n",
    "    ner_context = []\n",
    "    part_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0 and context_list[i] not in stop_words:\n",
    "            part_ner[i] = 'i_ner'\n",
    "            ner_context.append(context_list[i])\n",
    "\n",
    "    # combine lemma to ner_context list\n",
    "    ner_context_str = ' '.join(ner_context)\n",
    "    ner_context_ = nlp(ner_context_str)\n",
    "    ner_context_lemma = [w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in ner_context_]\n",
    "    ner_context_all = ner_context_lemma +  ner_context\n",
    "\n",
    "    for j in range(len(context_list)):\n",
    "        if context_list[j] in ner_context_all:\n",
    "            part_ner[j] = 'i_ner'\n",
    "    return part_ner\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"normalize spaces in a string.\"\"\"\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "nlp = None\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize spacy in each process\"\"\"\n",
    "    '''\n",
    "    'en': Noun chunks are \"base noun phrases\" â€“ flat phrases that have a noun as their head.\n",
    "    parser=False or disable=['parser'] : don't need any of the syntactic information,\n",
    "                                        and will make spaCy load and run much faster.\n",
    "    '''\n",
    "    global nlp\n",
    "    nlp = spacy.load('en', parser=False)\n",
    "\n",
    "def annotate(row):\n",
    "    '''\n",
    "    notice: the tagging feature only apply on context\n",
    "    '''\n",
    "    global nlp\n",
    "    id_, context, question = row[:3]\n",
    "    q_doc = nlp(clean_spaces(question))\n",
    "    c_doc = nlp(clean_spaces(context))\n",
    "    question_tokens = [normalize_text(w.text) for w in q_doc]\n",
    "    context_tokens = [normalize_text(w.text) for w in c_doc]\n",
    "    question_tokens_lower = [w.lower() for w in question_tokens]\n",
    "    context_tokens_lower = [w.lower() for w in context_tokens]\n",
    "    context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc] # the lenghth of each tokens\n",
    "    #for context the new features\n",
    "    context_tags = [w.tag_ for w in c_doc] # POS tagging\n",
    "    context_ents = [w.ent_type_ for w in c_doc] # NER tagging\n",
    "    context_iob_np = iob_np_tag(context_tags) # iob_np\n",
    "    context_iob_ner = iob_ner_tag(context_ents) #iob_ner\n",
    "    context_part_ner = part_ner_tag(context_ents, context_tokens) #part_ner\n",
    "\n",
    "    #for question the new features\n",
    "    question_tags = [w.tag_ for w in q_doc] # POS tagging\n",
    "    question_ents = [w.ent_type_ for w in q_doc] # NER tagging\n",
    "    question_iob_np = iob_np_tag(question_tags) # iob_np\n",
    "    question_iob_ner = iob_ner_tag(question_ents) #iob_ner\n",
    "\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in q_doc}\n",
    "    # PRON is such as me/it/you\n",
    "    # lemma_ : cats -> cat\n",
    "\n",
    "    question_tokens_set = set(question_tokens)\n",
    "    question_tokens_lower_set = set(question_tokens_lower)\n",
    "    match_origin = [w in question_tokens_set for w in context_tokens]\n",
    "    match_lower = [w in question_tokens_lower_set for w in context_tokens_lower]\n",
    "    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in c_doc]\n",
    "    # term frequency in document\n",
    "    counter_ = collections.Counter(context_tokens_lower)\n",
    "    total = len(context_tokens_lower)\n",
    "    # frequent feature\n",
    "    context_tf = [counter_[w] / total for w in context_tokens_lower]\n",
    "    # exact match feature refering to the paper\n",
    "    context_features = list(zip(match_origin, match_lower, match_lemma, context_tf))\n",
    "    \n",
    "    context_tokens = context_tokens_lower\n",
    "    question_tokens = question_tokens_lower\n",
    "    return (id_, context_tokens, context_features, context_tags, context_ents, context_iob_np, context_iob_ner, context_part_ner,\n",
    "            question_tags,question_ents,question_iob_np,question_iob_ner,\n",
    "            question_tokens, context, context_token_span) + row[3:]\n",
    "\n",
    "def index_answer(row):\n",
    "    token_span = row[-4] #context_token_span\n",
    "    starts, ends = zip(*token_span)\n",
    "    answer_start = row[-2]\n",
    "    answer_end = row[-1]\n",
    "    try:\n",
    "        return row[:-3] + (starts.index(answer_start), ends.index(answer_end))\n",
    "    except ValueError:\n",
    "        return row[:-3] + (None, None)\n",
    "nlp = spacy.load('en', parser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../SQuAD/meta.msgpack', 'rb') as f:\n",
    "    meta = msgpack.load(f, encoding='utf8')\n",
    "with open('../SQuAD/data.msgpack', 'rb') as f:\n",
    "    data = msgpack.load(f, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = '../SQuAD/dev-v1.1.json'\n",
    "dev = flatten_json(dev_file, 'dev')\n",
    "dev_y = [x[-1] for x in data['dev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denver Broncos', 'Denver Broncos', 'Denver Broncos']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unveil_detail(case_id):\n",
    "\n",
    "    # check if the id match \n",
    "    assert dev[case_id][0] == data['dev'][case_id][0]\n",
    "    dev_ann = annotate(dev[case_id])\n",
    "    context = dev_ann[1]\n",
    "    c_pos = dev_ann[3]\n",
    "    c_ner = dev_ann[4]\n",
    "    c_iob_np = dev_ann[5]\n",
    "    c_iob_ner = dev_ann[6]\n",
    "    c_part_ner = dev_ann[7]\n",
    "    q_pos = dev_ann[8]\n",
    "    q_ner = dev_ann[9]\n",
    "    q_iob_np = dev_ann[10]\n",
    "    q_iob_ner = dev_ann[11]\n",
    "    question = dev_ann[12]\n",
    "    context_para = dev_ann[13]\n",
    "    answer = dev_ann[15]\n",
    "    question_df = pd.DataFrame(np.column_stack([question,q_pos,q_iob_np,q_ner,q_iob_ner]))\n",
    "    context_df = pd.DataFrame(np.column_stack([context,c_pos,c_iob_np,c_ner,c_iob_ner, c_part_ner]))\n",
    "\n",
    "    return question_df, context_df, context_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df, context_df,context_para = unveil_detail(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0     1     2    3      4\n",
      "0        how   WRB  o_np       o_ner\n",
      "1       many    JJ  o_np       o_ner\n",
      "2      times   NNS  b_np       o_ner\n",
      "3       have   VBP  o_np       o_ner\n",
      "4        the    DT  o_np       o_ner\n",
      "5   panthers  NNPS  b_np  ORG  b_ner\n",
      "6       been   VBN  o_np       o_ner\n",
      "7         in    IN  o_np       o_ner\n",
      "8        the    DT  o_np       o_ner\n",
      "9      super   NNP  b_np  ORG  b_ner\n",
      "10      bowl   NNP  i_np  ORG  i_ner\n",
      "11         ?     .  o_np       o_ner\n"
     ]
    }
   ],
   "source": [
    "print(question_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Panthers finished the regular season with a 15â€“1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49â€“15 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12â€“4 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20â€“18 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.\n"
     ]
    }
   ],
   "source": [
    "print(context_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0      1     2         3      4      5\n",
      "0             the     DT  o_np            o_ner  o_ner\n",
      "1        panthers   NNPS  b_np            o_ner  o_ner\n",
      "2        finished    VBD  o_np            o_ner  o_ner\n",
      "3             the     DT  o_np            o_ner  o_ner\n",
      "4         regular     JJ  o_np            o_ner  o_ner\n",
      "5          season     NN  b_np            o_ner  o_ner\n",
      "6            with     IN  o_np            o_ner  o_ner\n",
      "7               a     DT  o_np            o_ner  o_ner\n",
      "8            15â€“1     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "9          record     NN  b_np            o_ner  o_ner\n",
      "10              ,      ,  o_np            o_ner  o_ner\n",
      "11            and     CC  o_np            o_ner  o_ner\n",
      "12    quarterback     NN  b_np            o_ner  o_ner\n",
      "13            cam    NNP  i_np    PERSON  b_ner  i_ner\n",
      "14         newton    NNP  i_np    PERSON  i_ner  i_ner\n",
      "15            was    VBD  o_np            o_ner  o_ner\n",
      "16          named    VBN  o_np            o_ner  o_ner\n",
      "17            the     DT  o_np            o_ner  o_ner\n",
      "18            nfl    NNP  b_np       ORG  b_ner  i_ner\n",
      "19           most    NNP  i_np            o_ner  o_ner\n",
      "20       valuable    NNP  i_np            o_ner  o_ner\n",
      "21         player    NNP  i_np    PERSON  b_ner  i_ner\n",
      "22              (  -LRB-  o_np            o_ner  o_ner\n",
      "23            mvp    NNP  b_np       ORG  b_ner  i_ner\n",
      "24              )  -RRB-  o_np            o_ner  o_ner\n",
      "25              .      .  o_np            o_ner  o_ner\n",
      "26           they    PRP  o_np            o_ner  o_ner\n",
      "27       defeated    VBD  o_np            o_ner  o_ner\n",
      "28            the     DT  o_np            o_ner  o_ner\n",
      "29        arizona    NNP  b_np      NORP  b_ner  i_ner\n",
      "30      cardinals   NNPS  i_np      NORP  i_ner  i_ner\n",
      "31          49â€“15     CD  o_np            o_ner  o_ner\n",
      "32             in     IN  o_np            o_ner  o_ner\n",
      "33            the     DT  o_np            o_ner  o_ner\n",
      "34            nfc    NNP  b_np       ORG  b_ner  i_ner\n",
      "35   championship    NNP  i_np       ORG  i_ner  i_ner\n",
      "36           game    NNP  i_np       ORG  i_ner  i_ner\n",
      "37            and     CC  o_np            o_ner  o_ner\n",
      "38       advanced     JJ  o_np            o_ner  o_ner\n",
      "39             to     IN  o_np            o_ner  o_ner\n",
      "40          their   PRP$  o_np            o_ner  o_ner\n",
      "41         second     JJ  o_np   ORDINAL  b_ner  i_ner\n",
      "42          super    NNP  b_np       ORG  i_ner  i_ner\n",
      "43           bowl    NNP  i_np       ORG  i_ner  i_ner\n",
      "44     appearance     NN  i_np            o_ner  o_ner\n",
      "45          since     IN  o_np            o_ner  o_ner\n",
      "46            the     DT  o_np            o_ner  o_ner\n",
      "47      franchise     NN  b_np            o_ner  o_ner\n",
      "48            was    VBD  o_np            o_ner  o_ner\n",
      "49        founded    VBN  o_np            o_ner  o_ner\n",
      "50             in     IN  o_np            o_ner  o_ner\n",
      "51           1995     CD  o_np      DATE  b_ner  i_ner\n",
      "52              .      .  o_np            o_ner  o_ner\n",
      "53            the     DT  o_np            o_ner  o_ner\n",
      "54        broncos   NNPS  b_np       ORG  b_ner  i_ner\n",
      "55       finished    VBD  o_np            o_ner  o_ner\n",
      "56            the     DT  o_np            o_ner  o_ner\n",
      "57        regular     JJ  o_np            o_ner  o_ner\n",
      "58         season     NN  b_np            o_ner  o_ner\n",
      "59           with     IN  o_np            o_ner  o_ner\n",
      "60              a     DT  o_np            o_ner  o_ner\n",
      "61           12â€“4     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "62         record     NN  b_np            o_ner  o_ner\n",
      "63              ,      ,  o_np            o_ner  o_ner\n",
      "64            and     CC  o_np            o_ner  o_ner\n",
      "65         denied    VBD  o_np            o_ner  o_ner\n",
      "66            the     DT  o_np       ORG  b_ner  o_ner\n",
      "67            new    NNP  b_np       ORG  i_ner  i_ner\n",
      "68        england    NNP  i_np       ORG  i_ner  i_ner\n",
      "69       patriots    NNP  i_np       ORG  i_ner  i_ner\n",
      "70              a     DT  o_np            o_ner  o_ner\n",
      "71         chance     NN  b_np            o_ner  o_ner\n",
      "72             to     TO  o_np            o_ner  o_ner\n",
      "73         defend     VB  o_np            o_ner  o_ner\n",
      "74          their   PRP$  o_np            o_ner  o_ner\n",
      "75          title     NN  b_np            o_ner  o_ner\n",
      "76           from     IN  o_np            o_ner  o_ner\n",
      "77          super    NNP  b_np       ORG  b_ner  i_ner\n",
      "78           bowl    NNP  i_np       ORG  i_ner  i_ner\n",
      "79           xlix    NNP  i_np       ORG  i_ner  i_ner\n",
      "80             by     IN  o_np            o_ner  o_ner\n",
      "81      defeating    VBG  o_np            o_ner  o_ner\n",
      "82           them    PRP  o_np            o_ner  o_ner\n",
      "83          20â€“18     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "84             in     IN  o_np            o_ner  o_ner\n",
      "85            the     DT  o_np            o_ner  o_ner\n",
      "86            afc    NNP  b_np       ORG  b_ner  i_ner\n",
      "87   championship    NNP  i_np       ORG  i_ner  i_ner\n",
      "88           game    NNP  i_np       ORG  i_ner  i_ner\n",
      "89              .      .  o_np            o_ner  o_ner\n",
      "90           they    PRP  o_np            o_ner  o_ner\n",
      "91         joined    VBD  o_np            o_ner  o_ner\n",
      "92            the     DT  o_np            o_ner  o_ner\n",
      "93       patriots   NNPS  b_np       ORG  b_ner  i_ner\n",
      "94              ,      ,  o_np            o_ner  o_ner\n",
      "95         dallas    NNP  b_np       ORG  b_ner  i_ner\n",
      "96        cowboys    NNP  i_np       ORG  i_ner  i_ner\n",
      "97              ,      ,  o_np            o_ner  o_ner\n",
      "98            and     CC  o_np            o_ner  o_ner\n",
      "99     pittsburgh    NNP  b_np       ORG  b_ner  i_ner\n",
      "100      steelers   NNPS  i_np       ORG  i_ner  i_ner\n",
      "101            as     IN  o_np            o_ner  o_ner\n",
      "102           one     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "103            of     IN  o_np            o_ner  o_ner\n",
      "104          four     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "105         teams    NNS  b_np            o_ner  o_ner\n",
      "106          that    WDT  o_np            o_ner  o_ner\n",
      "107          have    VBP  o_np            o_ner  o_ner\n",
      "108          made    VBN  o_np            o_ner  o_ner\n",
      "109         eight     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "110   appearances    NNS  b_np            o_ner  o_ner\n",
      "111            in     IN  o_np            o_ner  o_ner\n",
      "112           the     DT  o_np     EVENT  b_ner  o_ner\n",
      "113         super    NNP  b_np     EVENT  i_ner  i_ner\n",
      "114          bowl    NNP  i_np     EVENT  i_ner  i_ner\n",
      "115             .      .  o_np            o_ner  o_ner\n"
     ]
    }
   ],
   "source": [
    "print(context_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
