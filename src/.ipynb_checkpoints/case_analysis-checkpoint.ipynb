{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import msgpack\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(data_file, mode):\n",
    "    \"\"\"Flatten each article in training data.\"\"\"\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)['data']\n",
    "    rows = []\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                id_, question, answers = qa['id'], qa['question'], qa['answers']\n",
    "                if mode == 'train':\n",
    "                    answer = answers[0]['text']  # in training data there's only one answer\n",
    "                    answer_start = answers[0]['answer_start'] # char level length\n",
    "                    answer_end = answer_start + len(answer) # char level lenght\n",
    "                    rows.append((id_, context, question, answer, answer_start, answer_end))\n",
    "                else:  # mode == 'dev'\n",
    "                    answers = [a['text'] for a in answers]\n",
    "                    rows.append((id_, context, question, answers))\n",
    "    return rows\n",
    "\n",
    "def iob_np_tag(tag_list):\n",
    "    '''\n",
    "    function for creating iob_np\n",
    "    @in: a list of POS tags\n",
    "    @out: iob_np tags\n",
    "    '''\n",
    "    iob_np = ['o_np'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if 'NN' in tag_list[i]:\n",
    "            if iob_np[i-1] == 'b_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            elif iob_np[i-1] == 'i_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            else:\n",
    "                iob_np[i] = 'b_np'\n",
    "        i +=1\n",
    "    return iob_np\n",
    "\n",
    "## iob tag for NER\n",
    "def iob_ner_tag(tag_list):\n",
    "    '''\n",
    "    function for creating iob_ner\n",
    "    @in: a list of ner tags\n",
    "    @out: iob_ner tags\n",
    "    '''\n",
    "    iob_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0:\n",
    "            if iob_ner[i-1] == 'b_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            elif iob_ner[i-1] == 'i_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            else:\n",
    "                iob_ner[i] = 'b_ner'\n",
    "        i +=1\n",
    "    return iob_ner\n",
    "\n",
    "## Part of NER tag\n",
    "stop_words = ['a', 'an', 'the', 'of', 'for', '\\'s', 'For', 'The', 'A', 'An', ',', ':', '.', ' ,', ', ']\n",
    "def part_ner_tag(tag_list, context_list):\n",
    "    '''\n",
    "    @in: a list of ner tags\n",
    "    @out: part of ner tags\n",
    "    '''\n",
    "    ner_context = []\n",
    "    part_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0 and context_list[i] not in stop_words:\n",
    "            part_ner[i] = 'i_ner'\n",
    "            ner_context.append(context_list[i])\n",
    "\n",
    "    # combine lemma to ner_context list\n",
    "    ner_context_str = ' '.join(ner_context)\n",
    "    ner_context_ = nlp(ner_context_str)\n",
    "    ner_context_lemma = [w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in ner_context_]\n",
    "    ner_context_all = ner_context_lemma +  ner_context\n",
    "\n",
    "    for j in range(len(context_list)):\n",
    "        if context_list[j] in ner_context_all:\n",
    "            part_ner[j] = 'i_ner'\n",
    "    return part_ner\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"normalize spaces in a string.\"\"\"\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "nlp = None\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize spacy in each process\"\"\"\n",
    "    '''\n",
    "    'en': Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head.\n",
    "    parser=False or disable=['parser'] : don't need any of the syntactic information,\n",
    "                                        and will make spaCy load and run much faster.\n",
    "    '''\n",
    "    global nlp\n",
    "    nlp = spacy.load('en', parser=False)\n",
    "\n",
    "def annotate(row):\n",
    "    '''\n",
    "    notice: the tagging feature only apply on context\n",
    "    '''\n",
    "    global nlp\n",
    "    id_, context, question = row[:3]\n",
    "    q_doc = nlp(clean_spaces(question))\n",
    "    c_doc = nlp(clean_spaces(context))\n",
    "    question_tokens = [normalize_text(w.text) for w in q_doc]\n",
    "    context_tokens = [normalize_text(w.text) for w in c_doc]\n",
    "    question_tokens_lower = [w.lower() for w in question_tokens]\n",
    "    context_tokens_lower = [w.lower() for w in context_tokens]\n",
    "    context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc] # the lenghth of each tokens\n",
    "    #for context the new features\n",
    "    context_tags = [w.tag_ for w in c_doc] # POS tagging\n",
    "    context_ents = [w.ent_type_ for w in c_doc] # NER tagging\n",
    "    context_iob_np = iob_np_tag(context_tags) # iob_np\n",
    "    context_iob_ner = iob_ner_tag(context_ents) #iob_ner\n",
    "    context_part_ner = part_ner_tag(context_ents, context_tokens) #part_ner\n",
    "\n",
    "    #for question the new features\n",
    "    question_tags = [w.tag_ for w in q_doc] # POS tagging\n",
    "    question_ents = [w.ent_type_ for w in q_doc] # NER tagging\n",
    "    question_iob_np = iob_np_tag(question_tags) # iob_np\n",
    "    question_iob_ner = iob_ner_tag(question_ents) #iob_ner\n",
    "\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in q_doc}\n",
    "    # PRON is such as me/it/you\n",
    "    # lemma_ : cats -> cat\n",
    "\n",
    "    question_tokens_set = set(question_tokens)\n",
    "    question_tokens_lower_set = set(question_tokens_lower)\n",
    "    match_origin = [w in question_tokens_set for w in context_tokens]\n",
    "    match_lower = [w in question_tokens_lower_set for w in context_tokens_lower]\n",
    "    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in c_doc]\n",
    "    # term frequency in document\n",
    "    counter_ = collections.Counter(context_tokens_lower)\n",
    "    total = len(context_tokens_lower)\n",
    "    # frequent feature\n",
    "    context_tf = [counter_[w] / total for w in context_tokens_lower]\n",
    "    # exact match feature refering to the paper\n",
    "    context_features = list(zip(match_origin, match_lower, match_lemma, context_tf))\n",
    "    \n",
    "    context_tokens = context_tokens_lower\n",
    "    question_tokens = question_tokens_lower\n",
    "    return (id_, context_tokens, context_features, context_tags, context_ents, context_iob_np, context_iob_ner, context_part_ner,\n",
    "            question_tags,question_ents,question_iob_np,question_iob_ner,\n",
    "            question_tokens, context, context_token_span) + row[3:]\n",
    "\n",
    "def index_answer(row):\n",
    "    token_span = row[-4] #context_token_span\n",
    "    starts, ends = zip(*token_span)\n",
    "    answer_start = row[-2]\n",
    "    answer_end = row[-1]\n",
    "    try:\n",
    "        return row[:-3] + (starts.index(answer_start), ends.index(answer_end))\n",
    "    except ValueError:\n",
    "        return row[:-3] + (None, None)\n",
    "nlp = spacy.load('en', parser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../SQuAD/meta.msgpack', 'rb') as f:\n",
    "    meta = msgpack.load(f, encoding='utf8')\n",
    "with open('../SQuAD/data.msgpack', 'rb') as f:\n",
    "    data = msgpack.load(f, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = '../SQuAD/dev-v1.1.json'\n",
    "dev = flatten_json(dev_file, 'dev')\n",
    "dev_y = [x[-1] for x in data['dev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denver Broncos', 'Denver Broncos', 'Denver Broncos']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unveil_detail(case_id):\n",
    "\n",
    "    # check if the id match \n",
    "    assert dev[case_id][0] == data['dev'][case_id][0]\n",
    "    dev_ann = annotate(dev[case_id])\n",
    "    context = dev_ann[1]\n",
    "    c_pos = dev_ann[3]\n",
    "    c_ner = dev_ann[4]\n",
    "    c_iob_np = dev_ann[5]\n",
    "    c_iob_ner = dev_ann[6]\n",
    "    c_part_ner = dev_ann[7]\n",
    "    q_pos = dev_ann[8]\n",
    "    q_ner = dev_ann[9]\n",
    "    q_iob_np = dev_ann[10]\n",
    "    q_iob_ner = dev_ann[11]\n",
    "    question = dev_ann[12]\n",
    "    context_para = dev_ann[13]\n",
    "    answer = dev_ann[15]\n",
    "    question_df = pd.DataFrame(np.column_stack([question,q_pos,q_iob_np,q_ner,q_iob_ner]))\n",
    "    context_df = pd.DataFrame(np.column_stack([context,c_pos,c_iob_np,c_ner,c_iob_ner, c_part_ner]))\n",
    "\n",
    "    return question_df, context_df, context_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df, context_df,context_para = unveil_detail(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0     1     2    3      4\n",
      "0        how   WRB  o_np       o_ner\n",
      "1       many    JJ  o_np       o_ner\n",
      "2      times   NNS  b_np       o_ner\n",
      "3       have   VBP  o_np       o_ner\n",
      "4        the    DT  o_np       o_ner\n",
      "5   panthers  NNPS  b_np  ORG  b_ner\n",
      "6       been   VBN  o_np       o_ner\n",
      "7         in    IN  o_np       o_ner\n",
      "8        the    DT  o_np       o_ner\n",
      "9      super   NNP  b_np  ORG  b_ner\n",
      "10      bowl   NNP  i_np  ORG  i_ner\n",
      "11         ?     .  o_np       o_ner\n"
     ]
    }
   ],
   "source": [
    "print(question_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Panthers finished the regular season with a 15–1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49–15 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12–4 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20–18 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.\n"
     ]
    }
   ],
   "source": [
    "print(context_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0      1     2         3      4      5\n",
      "0             the     DT  o_np            o_ner  o_ner\n",
      "1        panthers   NNPS  b_np            o_ner  o_ner\n",
      "2        finished    VBD  o_np            o_ner  o_ner\n",
      "3             the     DT  o_np            o_ner  o_ner\n",
      "4         regular     JJ  o_np            o_ner  o_ner\n",
      "5          season     NN  b_np            o_ner  o_ner\n",
      "6            with     IN  o_np            o_ner  o_ner\n",
      "7               a     DT  o_np            o_ner  o_ner\n",
      "8            15–1     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "9          record     NN  b_np            o_ner  o_ner\n",
      "10              ,      ,  o_np            o_ner  o_ner\n",
      "11            and     CC  o_np            o_ner  o_ner\n",
      "12    quarterback     NN  b_np            o_ner  o_ner\n",
      "13            cam    NNP  i_np    PERSON  b_ner  i_ner\n",
      "14         newton    NNP  i_np    PERSON  i_ner  i_ner\n",
      "15            was    VBD  o_np            o_ner  o_ner\n",
      "16          named    VBN  o_np            o_ner  o_ner\n",
      "17            the     DT  o_np            o_ner  o_ner\n",
      "18            nfl    NNP  b_np       ORG  b_ner  i_ner\n",
      "19           most    NNP  i_np            o_ner  o_ner\n",
      "20       valuable    NNP  i_np            o_ner  o_ner\n",
      "21         player    NNP  i_np    PERSON  b_ner  i_ner\n",
      "22              (  -LRB-  o_np            o_ner  o_ner\n",
      "23            mvp    NNP  b_np       ORG  b_ner  i_ner\n",
      "24              )  -RRB-  o_np            o_ner  o_ner\n",
      "25              .      .  o_np            o_ner  o_ner\n",
      "26           they    PRP  o_np            o_ner  o_ner\n",
      "27       defeated    VBD  o_np            o_ner  o_ner\n",
      "28            the     DT  o_np            o_ner  o_ner\n",
      "29        arizona    NNP  b_np      NORP  b_ner  i_ner\n",
      "30      cardinals   NNPS  i_np      NORP  i_ner  i_ner\n",
      "31          49–15     CD  o_np            o_ner  o_ner\n",
      "32             in     IN  o_np            o_ner  o_ner\n",
      "33            the     DT  o_np            o_ner  o_ner\n",
      "34            nfc    NNP  b_np       ORG  b_ner  i_ner\n",
      "35   championship    NNP  i_np       ORG  i_ner  i_ner\n",
      "36           game    NNP  i_np       ORG  i_ner  i_ner\n",
      "37            and     CC  o_np            o_ner  o_ner\n",
      "38       advanced     JJ  o_np            o_ner  o_ner\n",
      "39             to     IN  o_np            o_ner  o_ner\n",
      "40          their   PRP$  o_np            o_ner  o_ner\n",
      "41         second     JJ  o_np   ORDINAL  b_ner  i_ner\n",
      "42          super    NNP  b_np       ORG  i_ner  i_ner\n",
      "43           bowl    NNP  i_np       ORG  i_ner  i_ner\n",
      "44     appearance     NN  i_np            o_ner  o_ner\n",
      "45          since     IN  o_np            o_ner  o_ner\n",
      "46            the     DT  o_np            o_ner  o_ner\n",
      "47      franchise     NN  b_np            o_ner  o_ner\n",
      "48            was    VBD  o_np            o_ner  o_ner\n",
      "49        founded    VBN  o_np            o_ner  o_ner\n",
      "50             in     IN  o_np            o_ner  o_ner\n",
      "51           1995     CD  o_np      DATE  b_ner  i_ner\n",
      "52              .      .  o_np            o_ner  o_ner\n",
      "53            the     DT  o_np            o_ner  o_ner\n",
      "54        broncos   NNPS  b_np       ORG  b_ner  i_ner\n",
      "55       finished    VBD  o_np            o_ner  o_ner\n",
      "56            the     DT  o_np            o_ner  o_ner\n",
      "57        regular     JJ  o_np            o_ner  o_ner\n",
      "58         season     NN  b_np            o_ner  o_ner\n",
      "59           with     IN  o_np            o_ner  o_ner\n",
      "60              a     DT  o_np            o_ner  o_ner\n",
      "61           12–4     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "62         record     NN  b_np            o_ner  o_ner\n",
      "63              ,      ,  o_np            o_ner  o_ner\n",
      "64            and     CC  o_np            o_ner  o_ner\n",
      "65         denied    VBD  o_np            o_ner  o_ner\n",
      "66            the     DT  o_np       ORG  b_ner  o_ner\n",
      "67            new    NNP  b_np       ORG  i_ner  i_ner\n",
      "68        england    NNP  i_np       ORG  i_ner  i_ner\n",
      "69       patriots    NNP  i_np       ORG  i_ner  i_ner\n",
      "70              a     DT  o_np            o_ner  o_ner\n",
      "71         chance     NN  b_np            o_ner  o_ner\n",
      "72             to     TO  o_np            o_ner  o_ner\n",
      "73         defend     VB  o_np            o_ner  o_ner\n",
      "74          their   PRP$  o_np            o_ner  o_ner\n",
      "75          title     NN  b_np            o_ner  o_ner\n",
      "76           from     IN  o_np            o_ner  o_ner\n",
      "77          super    NNP  b_np       ORG  b_ner  i_ner\n",
      "78           bowl    NNP  i_np       ORG  i_ner  i_ner\n",
      "79           xlix    NNP  i_np       ORG  i_ner  i_ner\n",
      "80             by     IN  o_np            o_ner  o_ner\n",
      "81      defeating    VBG  o_np            o_ner  o_ner\n",
      "82           them    PRP  o_np            o_ner  o_ner\n",
      "83          20–18     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "84             in     IN  o_np            o_ner  o_ner\n",
      "85            the     DT  o_np            o_ner  o_ner\n",
      "86            afc    NNP  b_np       ORG  b_ner  i_ner\n",
      "87   championship    NNP  i_np       ORG  i_ner  i_ner\n",
      "88           game    NNP  i_np       ORG  i_ner  i_ner\n",
      "89              .      .  o_np            o_ner  o_ner\n",
      "90           they    PRP  o_np            o_ner  o_ner\n",
      "91         joined    VBD  o_np            o_ner  o_ner\n",
      "92            the     DT  o_np            o_ner  o_ner\n",
      "93       patriots   NNPS  b_np       ORG  b_ner  i_ner\n",
      "94              ,      ,  o_np            o_ner  o_ner\n",
      "95         dallas    NNP  b_np       ORG  b_ner  i_ner\n",
      "96        cowboys    NNP  i_np       ORG  i_ner  i_ner\n",
      "97              ,      ,  o_np            o_ner  o_ner\n",
      "98            and     CC  o_np            o_ner  o_ner\n",
      "99     pittsburgh    NNP  b_np       ORG  b_ner  i_ner\n",
      "100      steelers   NNPS  i_np       ORG  i_ner  i_ner\n",
      "101            as     IN  o_np            o_ner  o_ner\n",
      "102           one     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "103            of     IN  o_np            o_ner  o_ner\n",
      "104          four     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "105         teams    NNS  b_np            o_ner  o_ner\n",
      "106          that    WDT  o_np            o_ner  o_ner\n",
      "107          have    VBP  o_np            o_ner  o_ner\n",
      "108          made    VBN  o_np            o_ner  o_ner\n",
      "109         eight     CD  o_np  CARDINAL  b_ner  i_ner\n",
      "110   appearances    NNS  b_np            o_ner  o_ner\n",
      "111            in     IN  o_np            o_ner  o_ner\n",
      "112           the     DT  o_np     EVENT  b_ner  o_ner\n",
      "113         super    NNP  b_np     EVENT  i_ner  i_ner\n",
      "114          bowl    NNP  i_np     EVENT  i_ner  i_ner\n",
      "115             .      .  o_np            o_ner  o_ner\n"
     ]
    }
   ],
   "source": [
    "print(context_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
