{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import msgpack\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(data_file, mode):\n",
    "    \"\"\"Flatten each article in training data.\"\"\"\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)['data']\n",
    "    rows = []\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                id_, question, answers = qa['id'], qa['question'], qa['answers']\n",
    "                if mode == 'train':\n",
    "                    answer = answers[0]['text']  # in training data there's only one answer\n",
    "                    answer_start = answers[0]['answer_start'] # char level length\n",
    "                    answer_end = answer_start + len(answer) # char level lenght\n",
    "                    rows.append((id_, context, question, answer, answer_start, answer_end))\n",
    "                else:  # mode == 'dev'\n",
    "                    answers = [a['text'] for a in answers]\n",
    "                    rows.append((id_, context, question, answers))\n",
    "    return rows\n",
    "\n",
    "def iob_np_tag(tag_list):\n",
    "    '''\n",
    "    function for creating iob_np\n",
    "    @in: a list of POS tags\n",
    "    @out: iob_np tags\n",
    "    '''\n",
    "    iob_np = ['o_np'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if 'NN' in tag_list[i]:\n",
    "            if iob_np[i-1] == 'b_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            elif iob_np[i-1] == 'i_np':\n",
    "                iob_np[i] = 'i_np'\n",
    "            else:\n",
    "                iob_np[i] = 'b_np'\n",
    "        i +=1\n",
    "    return iob_np\n",
    "\n",
    "## iob tag for NER\n",
    "def iob_ner_tag(tag_list):\n",
    "    '''\n",
    "    function for creating iob_ner\n",
    "    @in: a list of ner tags\n",
    "    @out: iob_ner tags\n",
    "    '''\n",
    "    iob_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0:\n",
    "            if iob_ner[i-1] == 'b_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            elif iob_ner[i-1] == 'i_ner':\n",
    "                iob_ner[i] = 'i_ner'\n",
    "            else:\n",
    "                iob_ner[i] = 'b_ner'\n",
    "        i +=1\n",
    "    return iob_ner\n",
    "\n",
    "## Part of NER tag\n",
    "stop_words = ['a', 'an', 'the', 'of', 'for', '\\'s', 'For', 'The', 'A', 'An', ',', ':', '.', ' ,', ', ']\n",
    "def part_ner_tag(tag_list, context_list):\n",
    "    '''\n",
    "    @in: a list of ner tags\n",
    "    @out: part of ner tags\n",
    "    '''\n",
    "    ner_context = []\n",
    "    part_ner = ['o_ner'] * len(tag_list)\n",
    "    for i in range(len(tag_list)):\n",
    "        if len(tag_list[i]) != 0 and context_list[i] not in stop_words:\n",
    "            part_ner[i] = 'i_ner'\n",
    "            ner_context.append(context_list[i])\n",
    "\n",
    "    # combine lemma to ner_context list\n",
    "    ner_context_str = ' '.join(ner_context)\n",
    "    ner_context_ = nlp(ner_context_str)\n",
    "    ner_context_lemma = [w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in ner_context_]\n",
    "    ner_context_all = ner_context_lemma +  ner_context\n",
    "\n",
    "    for j in range(len(context_list)):\n",
    "        if context_list[j] in ner_context_all:\n",
    "            part_ner[j] = 'i_ner'\n",
    "    return part_ner\n",
    "\n",
    "def clean_spaces(text):\n",
    "    \"\"\"normalize spaces in a string.\"\"\"\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "nlp = None\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize spacy in each process\"\"\"\n",
    "    '''\n",
    "    'en': Noun chunks are \"base noun phrases\" – flat phrases that have a noun as their head.\n",
    "    parser=False or disable=['parser'] : don't need any of the syntactic information,\n",
    "                                        and will make spaCy load and run much faster.\n",
    "    '''\n",
    "    global nlp\n",
    "    nlp = spacy.load('en', parser=False)\n",
    "\n",
    "def annotate(row):\n",
    "    '''\n",
    "    notice: the tagging feature only apply on context\n",
    "    '''\n",
    "    global nlp\n",
    "    id_, context, question = row[:3]\n",
    "    q_doc = nlp(clean_spaces(question))\n",
    "    c_doc = nlp(clean_spaces(context))\n",
    "    question_tokens = [normalize_text(w.text) for w in q_doc]\n",
    "    context_tokens = [normalize_text(w.text) for w in c_doc]\n",
    "    question_tokens_lower = [w.lower() for w in question_tokens]\n",
    "    context_tokens_lower = [w.lower() for w in context_tokens]\n",
    "    context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc] # the lenghth of each tokens\n",
    "    #for context the new features\n",
    "    context_tags = [w.tag_ for w in c_doc] # POS tagging\n",
    "    context_ents = [w.ent_type_ for w in c_doc] # NER tagging\n",
    "    context_iob_np = iob_np_tag(context_tags) # iob_np\n",
    "    context_iob_ner = iob_ner_tag(context_ents) #iob_ner\n",
    "    context_part_ner = part_ner_tag(context_ents, context_tokens) #part_ner\n",
    "\n",
    "    #for question the new features\n",
    "    question_tags = [w.tag_ for w in q_doc] # POS tagging\n",
    "    question_ents = [w.ent_type_ for w in q_doc] # NER tagging\n",
    "    question_iob_np = iob_np_tag(question_tags) # iob_np\n",
    "    question_iob_ner = iob_ner_tag(question_ents) #iob_ner\n",
    "\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in q_doc}\n",
    "    # PRON is such as me/it/you\n",
    "    # lemma_ : cats -> cat\n",
    "\n",
    "    question_tokens_set = set(question_tokens)\n",
    "    question_tokens_lower_set = set(question_tokens_lower)\n",
    "    match_origin = [w in question_tokens_set for w in context_tokens]\n",
    "    match_lower = [w in question_tokens_lower_set for w in context_tokens_lower]\n",
    "    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in c_doc]\n",
    "    # term frequency in document\n",
    "    counter_ = collections.Counter(context_tokens_lower)\n",
    "    total = len(context_tokens_lower)\n",
    "    # frequent feature\n",
    "    context_tf = [counter_[w] / total for w in context_tokens_lower]\n",
    "    # exact match feature refering to the paper\n",
    "    context_features = list(zip(match_origin, match_lower, match_lemma, context_tf))\n",
    "    \n",
    "    context_tokens = context_tokens_lower\n",
    "    question_tokens = question_tokens_lower\n",
    "    return (id_, context_tokens, context_features, context_tags, context_ents, context_iob_np, context_iob_ner, context_part_ner,\n",
    "            question_tags,question_ents,question_iob_np,question_iob_ner,\n",
    "            question_tokens, context, context_token_span) + row[3:]\n",
    "\n",
    "def index_answer(row):\n",
    "    token_span = row[-4] #context_token_span\n",
    "    starts, ends = zip(*token_span)\n",
    "    answer_start = row[-2]\n",
    "    answer_end = row[-1]\n",
    "    try:\n",
    "        return row[:-3] + (starts.index(answer_start), ends.index(answer_end))\n",
    "    except ValueError:\n",
    "        return row[:-3] + (None, None)\n",
    "nlp = spacy.load('en', parser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../SQuAD/meta.msgpack', 'rb') as f:\n",
    "    meta = msgpack.load(f, encoding='utf8')\n",
    "with open('../SQuAD/data.msgpack', 'rb') as f:\n",
    "    data = msgpack.load(f, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = '../SQuAD/dev-v1.1.json'\n",
    "dev = flatten_json(dev_file, 'dev')\n",
    "dev_y = [x[-1] for x in data['dev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unveil_detail(case_id):\n",
    "\n",
    "    # check if the id match \n",
    "    assert dev[case_id][0] == data['dev'][case_id][0]\n",
    "    dev_ann = annotate(dev[case_id])\n",
    "    context = dev_ann[1]\n",
    "    c_pos = dev_ann[3]\n",
    "    c_ner = dev_ann[4]\n",
    "    c_iob_np = dev_ann[5]\n",
    "    c_iob_ner = dev_ann[6]\n",
    "    c_part_ner = dev_ann[7]\n",
    "    q_pos = dev_ann[8]\n",
    "    q_ner = dev_ann[9]\n",
    "    q_iob_np = dev_ann[10]\n",
    "    q_iob_ner = dev_ann[11]\n",
    "    question = dev_ann[12]\n",
    "    context_para = dev_ann[13]\n",
    "    answer = dev_ann[15]\n",
    "    question_df = pd.DataFrame(np.column_stack([question,q_pos,q_iob_np,q_ner,q_iob_ner]))\n",
    "    context_df = pd.DataFrame(np.column_stack([context,c_pos,c_iob_np,c_ner,c_iob_ner, c_part_ner]))\n",
    "\n",
    "    return question_df, context_df, context_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df, context_df,context_para = unveil_detail(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0    1     2    3      4\n",
      "0         which  WDT  o_np       o_ner\n",
      "1           nfl  NNP  b_np  ORG  b_ner\n",
      "2          team   NN  i_np       o_ner\n",
      "3   represented  VBD  o_np       o_ner\n",
      "4           the   DT  o_np       o_ner\n",
      "5           afc  NNP  b_np       o_ner\n",
      "6            at   IN  o_np       o_ner\n",
      "7         super  NNP  b_np  ORG  b_ner\n",
      "8          bowl  NNP  i_np  ORG  i_ner\n",
      "9            50   CD  o_np       o_ner\n",
      "10            ?    .  o_np       o_ner\n"
     ]
    }
   ],
   "source": [
    "print(question_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
     ]
    }
   ],
   "source": [
    "print(context_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0      1     2            3      4      5\n",
      "0          super    NNP  b_np          ORG  b_ner  i_ner\n",
      "1           bowl    NNP  i_np          ORG  i_ner  i_ner\n",
      "2             50     CD  o_np               o_ner  i_ner\n",
      "3            was    VBD  o_np               o_ner  o_ner\n",
      "4             an     DT  o_np               o_ner  o_ner\n",
      "5       american     JJ  o_np         NORP  b_ner  i_ner\n",
      "6       football     NN  b_np               o_ner  i_ner\n",
      "7           game     NN  i_np               o_ner  o_ner\n",
      "8             to     TO  o_np               o_ner  o_ner\n",
      "9      determine     VB  o_np               o_ner  o_ner\n",
      "10           the     DT  o_np               o_ner  o_ner\n",
      "11      champion     NN  b_np               o_ner  o_ner\n",
      "12            of     IN  o_np               o_ner  o_ner\n",
      "13           the     DT  o_np          ORG  b_ner  o_ner\n",
      "14      national    NNP  b_np          ORG  i_ner  i_ner\n",
      "15      football    NNP  i_np          ORG  i_ner  i_ner\n",
      "16        league    NNP  i_np          ORG  i_ner  i_ner\n",
      "17             (  -LRB-  o_np               o_ner  o_ner\n",
      "18           nfl    NNP  b_np          ORG  b_ner  i_ner\n",
      "19             )  -RRB-  o_np               o_ner  o_ner\n",
      "20           for     IN  o_np               o_ner  o_ner\n",
      "21           the     DT  o_np               o_ner  o_ner\n",
      "22          2015     CD  o_np         DATE  b_ner  i_ner\n",
      "23        season     NN  b_np               o_ner  o_ner\n",
      "24             .      .  o_np               o_ner  o_ner\n",
      "25           the     DT  o_np          ORG  b_ner  o_ner\n",
      "26      american    NNP  b_np          ORG  i_ner  i_ner\n",
      "27      football    NNP  i_np          ORG  i_ner  i_ner\n",
      "28    conference    NNP  i_np          ORG  i_ner  i_ner\n",
      "29             (  -LRB-  o_np               o_ner  o_ner\n",
      "30           afc    NNP  b_np          ORG  b_ner  i_ner\n",
      "31             )  -RRB-  o_np               o_ner  o_ner\n",
      "32      champion     NN  b_np               o_ner  o_ner\n",
      "33        denver    NNP  i_np       PERSON  b_ner  i_ner\n",
      "34       broncos    NNP  i_np       PERSON  i_ner  i_ner\n",
      "35      defeated    VBD  o_np               o_ner  o_ner\n",
      "36           the     DT  o_np          ORG  b_ner  o_ner\n",
      "37      national    NNP  b_np          ORG  i_ner  i_ner\n",
      "38      football    NNP  i_np          ORG  i_ner  i_ner\n",
      "39    conference    NNP  i_np          ORG  i_ner  i_ner\n",
      "40             (  -LRB-  o_np               o_ner  o_ner\n",
      "41           nfc    NNP  b_np          ORG  b_ner  i_ner\n",
      "42             )  -RRB-  o_np               o_ner  o_ner\n",
      "43      champion     NN  b_np               o_ner  o_ner\n",
      "44      carolina    NNP  i_np          ORG  b_ner  i_ner\n",
      "45      panthers    NNP  i_np          ORG  i_ner  i_ner\n",
      "46         24–10     CD  o_np               o_ner  o_ner\n",
      "47            to     TO  o_np               o_ner  o_ner\n",
      "48          earn     VB  o_np               o_ner  o_ner\n",
      "49         their   PRP$  o_np               o_ner  o_ner\n",
      "50         third     JJ  o_np      ORDINAL  b_ner  i_ner\n",
      "51         super    NNP  b_np       PERSON  i_ner  i_ner\n",
      "52          bowl    NNP  i_np       PERSON  i_ner  i_ner\n",
      "53         title     NN  i_np               o_ner  o_ner\n",
      "54             .      .  o_np               o_ner  o_ner\n",
      "55           the     DT  o_np               o_ner  o_ner\n",
      "56          game     NN  b_np               o_ner  o_ner\n",
      "57           was    VBD  o_np               o_ner  o_ner\n",
      "58        played    VBN  o_np               o_ner  o_ner\n",
      "59            on     IN  o_np               o_ner  o_ner\n",
      "60      february    NNP  b_np         DATE  b_ner  i_ner\n",
      "61             7     CD  o_np         DATE  i_ner  i_ner\n",
      "62             ,      ,  o_np         DATE  i_ner  o_ner\n",
      "63          2016     CD  o_np         DATE  i_ner  i_ner\n",
      "64             ,      ,  o_np               o_ner  o_ner\n",
      "65            at     IN  o_np               o_ner  o_ner\n",
      "66          levi    NNP  b_np       PERSON  b_ner  i_ner\n",
      "67            's    POS  o_np               o_ner  o_ner\n",
      "68       stadium    NNP  b_np               o_ner  o_ner\n",
      "69            in     IN  o_np               o_ner  o_ner\n",
      "70           the     DT  o_np               o_ner  o_ner\n",
      "71           san    NNP  b_np          LOC  b_ner  i_ner\n",
      "72     francisco    NNP  i_np          LOC  i_ner  i_ner\n",
      "73           bay    NNP  i_np          LOC  i_ner  i_ner\n",
      "74          area    NNP  i_np          LOC  i_ner  i_ner\n",
      "75            at     IN  o_np               o_ner  o_ner\n",
      "76         santa    NNP  b_np          GPE  b_ner  i_ner\n",
      "77         clara    NNP  i_np          GPE  i_ner  i_ner\n",
      "78             ,      ,  o_np               o_ner  o_ner\n",
      "79    california    NNP  b_np          GPE  b_ner  i_ner\n",
      "80             .      .  o_np               o_ner  o_ner\n",
      "81            as     IN  o_np               o_ner  o_ner\n",
      "82          this     DT  o_np               o_ner  o_ner\n",
      "83           was    VBD  o_np               o_ner  o_ner\n",
      "84           the     DT  o_np               o_ner  o_ner\n",
      "85          50th     NN  b_np      ORDINAL  b_ner  i_ner\n",
      "86         super    NNP  i_np          ORG  i_ner  i_ner\n",
      "87          bowl    NNP  i_np          ORG  i_ner  i_ner\n",
      "88             ,      ,  o_np               o_ner  o_ner\n",
      "89           the     DT  o_np               o_ner  o_ner\n",
      "90        league     NN  b_np               o_ner  i_ner\n",
      "91    emphasized    VBD  o_np               o_ner  o_ner\n",
      "92           the     DT  o_np               o_ner  o_ner\n",
      "93             \"     ``  o_np               o_ner  o_ner\n",
      "94        golden     JJ  o_np               o_ner  o_ner\n",
      "95   anniversary     NN  b_np               o_ner  o_ner\n",
      "96             \"     ''  o_np               o_ner  o_ner\n",
      "97          with     IN  o_np               o_ner  o_ner\n",
      "98       various     JJ  o_np               o_ner  o_ner\n",
      "99          gold     NN  b_np               o_ner  o_ner\n",
      "100            -   HYPH  o_np               o_ner  o_ner\n",
      "101       themed    VBN  o_np               o_ner  o_ner\n",
      "102  initiatives    NNS  b_np               o_ner  o_ner\n",
      "103            ,      ,  o_np               o_ner  o_ner\n",
      "104           as     RB  o_np               o_ner  o_ner\n",
      "105         well     RB  o_np               o_ner  o_ner\n",
      "106           as     IN  o_np               o_ner  o_ner\n",
      "107  temporarily     RB  o_np               o_ner  o_ner\n",
      "108   suspending    VBG  o_np               o_ner  o_ner\n",
      "109          the     DT  o_np               o_ner  o_ner\n",
      "110    tradition     NN  b_np               o_ner  o_ner\n",
      "111           of     IN  o_np               o_ner  o_ner\n",
      "112       naming    VBG  o_np               o_ner  o_ner\n",
      "113         each     DT  o_np        EVENT  b_ner  i_ner\n",
      "114        super    NNP  b_np        EVENT  i_ner  i_ner\n",
      "115         bowl    NNP  i_np        EVENT  i_ner  i_ner\n",
      "116         game     NN  i_np               o_ner  o_ner\n",
      "117         with     IN  o_np               o_ner  o_ner\n",
      "118        roman     JJ  o_np         NORP  b_ner  i_ner\n",
      "119     numerals    NNS  b_np               o_ner  o_ner\n",
      "120            (  -LRB-  o_np               o_ner  o_ner\n",
      "121        under     IN  o_np               o_ner  o_ner\n",
      "122        which    WDT  o_np               o_ner  o_ner\n",
      "123          the     DT  o_np               o_ner  o_ner\n",
      "124         game     NN  b_np               o_ner  o_ner\n",
      "125        would     MD  o_np               o_ner  o_ner\n",
      "126         have     VB  o_np               o_ner  o_ner\n",
      "127         been    VBN  o_np               o_ner  o_ner\n",
      "128        known    VBN  o_np               o_ner  o_ner\n",
      "129           as     IN  o_np               o_ner  o_ner\n",
      "130            \"     ``  o_np               o_ner  o_ner\n",
      "131        super    NNP  b_np  WORK_OF_ART  b_ner  i_ner\n",
      "132         bowl    NNP  i_np  WORK_OF_ART  i_ner  i_ner\n",
      "133            l    NNP  i_np  WORK_OF_ART  i_ner  i_ner\n",
      "134            \"     ''  o_np               o_ner  o_ner\n",
      "135            )  -RRB-  o_np               o_ner  o_ner\n",
      "136            ,      ,  o_np               o_ner  o_ner\n",
      "137           so     IN  o_np               o_ner  o_ner\n",
      "138         that     IN  o_np               o_ner  o_ner\n",
      "139          the     DT  o_np               o_ner  o_ner\n",
      "140         logo     NN  b_np               o_ner  o_ner\n",
      "141        could     MD  o_np               o_ner  o_ner\n",
      "142  prominently     RB  o_np               o_ner  o_ner\n",
      "143      feature     VB  o_np               o_ner  o_ner\n",
      "144          the     DT  o_np               o_ner  o_ner\n",
      "145       arabic     JJ  o_np         NORP  b_ner  i_ner\n",
      "146     numerals    NNS  b_np               o_ner  o_ner\n",
      "147           50     CD  o_np     CARDINAL  b_ner  i_ner\n",
      "148            .      .  o_np               o_ner  o_ner\n"
     ]
    }
   ],
   "source": [
    "print(context_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison for start position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prediction results\n",
    "with open('../results/iob_result.txt') as results:\n",
    "    iob_result_list = json.load(results)\n",
    "with open('../results/posner_2_result.txt') as results:\n",
    "    posner_result_list = json.load(results)\n",
    "with open('../results/all_with_q_2_result.txt') as results:\n",
    "    all_with_q_result_list = json.load(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_start_correct(result):\n",
    "    correct = 0\n",
    "    for case_id in range(len(result)):\n",
    "        true_start_answer_list = [\"\".join(result[case_id]['true_answer'][i]).split()[0] for i in range(len(result[case_id]['true_answer']))]\n",
    "        pred_answer = \"\".join(result[case_id]['predict_answer']).split()\n",
    "        if len(pred_answer) != 0:\n",
    "            pred_start_answer = pred_answer[0]\n",
    "        else:\n",
    "            pred_start_answer = 'errrrroooorrrr'\n",
    "        if pred_start_answer in true_start_answer_list:\n",
    "            correct += 1\n",
    "    return correct/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7357615894039735"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_start_correct(iob_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison for EM score of NER/NP answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ner_case_list = []\n",
    "for case_id in range(len(iob_result_list)):\n",
    "    \n",
    "    ann_answer = [nlp(clean_spaces(answer)) for answer in iob_result_list[case_id]['true_answer']]\n",
    "    \n",
    "    pos_list = []\n",
    "    ner_list = []\n",
    "    for an in ann_answer:\n",
    "        pos_tags = [w.tag_ for w in an] \n",
    "        pos_list.append(pos_tags)\n",
    "        ner_ents = [w.ent_type_ for w in an]\n",
    "        ner_list.append(ner_ents)    \n",
    "\n",
    "    for pos_item in pos_list:\n",
    "        if False in ['NN' in pos for pos in pos_item]:\n",
    "            pass\n",
    "        else:\n",
    "            np_ner_case_list.append(case_id)\n",
    "            \n",
    "    for ner_item in ner_list:\n",
    "        if False in [len(ner)!=0 for ner in ner_item]:\n",
    "            pass\n",
    "        else:\n",
    "            np_ner_case_list.append(case_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ner_case_list = list(set(np_ner_case_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6343"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np_ner_case_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_posner_case_score(result):\n",
    "    em = 0\n",
    "    f1 = 0\n",
    "    for case_id in np_ner_case_list:\n",
    "        if result[case_id]['predict_answer'] in result[case_id]['true_answer']:\n",
    "            em += 1 \n",
    "    return em/len(np_ner_case_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7384518366703453"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_posner_case_score(iob_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463345420148195"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_posner_case_score(all_with_q_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
